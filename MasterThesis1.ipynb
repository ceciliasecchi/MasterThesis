{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MasterThesis1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNHsypUB8Qg4ixRZQ+STnkp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ceciliasecchi/MasterThesis/blob/main/MasterThesis1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-process of the data"
      ],
      "metadata": {
        "id": "gviBKEvlWE02"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biVdvmb1VfKA",
        "outputId": "3ebb9e04-c8df-4564-dca0-cf83075ee51a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.7/dist-packages (3.0.1)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (3.3.0)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (1.0.4)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (6.1.1)\n",
            "Requirement already satisfied: regex>=2020.04.04 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (2022.6.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy>=6.1->wordfreq) (0.2.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import bernoulli \n",
        "from scipy.special import softmax as scipy_softmax\n",
        "import pandas as pd\n",
        "!pip install wordfreq\n",
        "from wordfreq import word_frequency\n",
        "from nltk.collections import OrderedDict\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re  # For preprocessing\n",
        "import string\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import csv\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "from gensim.parsing.preprocessing import remove_stopwords,preprocess_string,preprocess_documents\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "from typing import DefaultDict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reads ‘tweets.txt’ file\n",
        "sample = open(\"tweets.txt\", \"r\")\n",
        "s = sample.read()\n",
        "\n",
        "stopp = {'and','hes','but','this','said','you','the','how','for','like','thats',\n",
        "         'like','thought','again','but','dont','that','ive','didnt','they','she',\n",
        "         'while','instead','would','thats','anything','its','hasnt','still',\n",
        "         'gets','went',\"wouldnt\",\"get\",\"let\",'way',\"these\",\"those\"\n",
        "         'looked','came','not','got','then','ive',\"were\",'there','wont','your'}\n",
        "voc = {}\n",
        "vocGlove = OrderedDict()\n",
        "vocGloveReverse = {}\n",
        "with open('vocGlove_tweets.csv', mode='r') as inp:\n",
        "    reader = csv.reader(inp)\n",
        "    for row in reader:\n",
        "      voc = {rows[0]:rows[1].replace(\"\\n\", \" \").replace(\"[\",\" \").replace(\"]\",\" \") for rows in reader}\n",
        "    \n",
        "for key,value in voc.items():\n",
        "  value=value.split()\n",
        "  if len(value)!=50:\n",
        "    print(key)\n",
        "  if key not in stopp:\n",
        "    vocGlove[key]=np.array(list(map(float,value)))\n",
        "\n",
        "## normalize the vectors if needed\n",
        "#for k,v in vocGlove.items():\n",
        "#  v = v/np.linalg.norm(v)\n",
        "#  #print(np.linalg.norm(v))\n",
        "#  vocGlove[k] = v\n",
        "#  vocGloveReverse[str(v)]=k\n",
        "\n",
        "dizionario=defaultdict(int)\n",
        "# Replaces escape character with space\n",
        "\n",
        "f = s.split(\"\\n\")\n",
        "data = []\n",
        "# iterate through each sentence in the file\n",
        "frasi=[]\n",
        "for i in f:\n",
        "  copia=i\n",
        "  i = remove_stopwords(i)\n",
        "  i = i.translate(i.maketrans('', '', string.punctuation+\"\\—\\-\\\"\\'\\“\\”\\’\\‘\"))\n",
        "  temp = []\n",
        "\t\n",
        "\t# tokenize the sentence into words\n",
        "  for j in word_tokenize(i):\n",
        "    j = j.lower()\n",
        "    if j in vocGlove.keys() and j not in stopp and len(j)>2:\n",
        "      dizionario[j] += 1\n",
        "      temp.append(j)\n",
        "    else: pass\n",
        "  if len(temp)>5:\n",
        "    data.append(temp)\n",
        "    frasi.append(copia)\n",
        "\n"
      ],
      "metadata": {
        "id": "9gCz9rHwV1yE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dictionary_data(text):\n",
        "    '''\n",
        "    the function given the text (in the form of lists of words, and each list corresponds to a sentence) generates:\n",
        "    - word_to_index: dictionary that has as keys the words and as values the position of the word.\n",
        "    - index_to_word: the inverse of word_to_index\n",
        "    - corpus: the text as a unique list of words\n",
        "    - vocab_size: the dimension of the vocabulary used in the text\n",
        "    - lenght_of_corpus: the total number of words of the text  \n",
        "    '''\n",
        "\n",
        "    word_to_index= OrderedDict()\n",
        "    index_to_word = OrderedDict()\n",
        "    corpus = []\n",
        "    corpus_sent = []\n",
        "    count = 0\n",
        "    vocab_size = 0\n",
        "    \n",
        "    for row in text:\n",
        "        for word in row:\n",
        "            word = word.lower()\n",
        "            corpus.append(word)\n",
        "            if word_to_index.get(word) == None:\n",
        "                word_to_index.update ( {word : count})\n",
        "                index_to_word.update ( {count : word })\n",
        "                count  += 1\n",
        "    vocab_size = len(word_to_index)\n",
        "    length_of_corpus = len(corpus)\n",
        "    return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus"
      ],
      "metadata": {
        "id": "P4hRmX2VWBRa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index,index_to_word,corpus,vocab_size,length_of_corpus=generate_dictionary_data(data)"
      ],
      "metadata": {
        "id": "w2ImViLwWDnK"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#build TF-IDF"
      ],
      "metadata": {
        "id": "SosiQpEXXwJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTF(sentence):\n",
        "  pesi_tf=[]\n",
        "  freq=DefaultDict(int)\n",
        "  bagOfWordsCount = len(set(sentence))\n",
        "  for word in sentence:\n",
        "    freq[word] += 1\n",
        "  for word in sentence:\n",
        "    pesi_tf.append(freq[word]/float(bagOfWordsCount))\n",
        "  return pesi_tf\n",
        "\n",
        "def computeIDF(text, corpus):\n",
        "  word_count={}\n",
        "  tot_sent=len(text)\n",
        "  for word in set(corpus):\n",
        "    word_count[word]=1\n",
        "    for sentence in text:\n",
        "      if word in sentence:\n",
        "        word_count[word] += 1\n",
        "    word_count[word]=np.log(tot_sent/word_count[word])\n",
        "  return word_count\n",
        "\n",
        "def computeTfIdf(text,corpus):\n",
        "  word_count=computeIDF(text,corpus)\n",
        "  pesi_tot=[]\n",
        "  for sentence in text:\n",
        "    pesi_tf = computeTF(sentence)\n",
        "    lenS=len(sentence)\n",
        "    for i in range(lenS):\n",
        "      pesi_tf[i] = pesi_tf[i]*word_count[sentence[i]]\n",
        "    pesi_tot.append(pesi_tf)\n",
        "  return pesi_tot\n",
        "\n"
      ],
      "metadata": {
        "id": "s6c2_29iXz5H"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"len corpus:\"+str(length_of_corpus))\n",
        "print(\"vocab size corpus:\"+str(vocab_size))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1G-gGOHX50H",
        "outputId": "5e14217a-94b5-41cd-f42d-0b553ef4165a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len corpus:1932\n",
            "vocab size corpus:1309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1st way to get a vector from a sentence: \n",
        "doing the average of the word2vec that compose it.\n"
      ],
      "metadata": {
        "id": "YMcscnoRYBgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence2vec={}\n",
        "sentencesAsVec=[]\n",
        "num_sent=len(data)\n",
        "\n",
        "count=0\n",
        "conteggio={}\n",
        "for i in range(num_sent):\n",
        "  len_sent=len(data[i])\n",
        "  vettori=[]\n",
        "  for parola in corpus[count:count+len_sent]:\n",
        "    vettori.append(vocGlove[parola])\n",
        "  conteggio[i]=[count,count+len_sent]\n",
        "  count += len_sent\n",
        "  sentencesAsVec.append(vettori)\n",
        "  sentence2vec[i]= np.mean(vettori,axis=0)\n"
      ],
      "metadata": {
        "id": "5mhDS9btYDxC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "massimi=[]\n",
        "for i in range(num_sent):\n",
        "  sentence=sentencesAsVec[i]\n",
        "  sentence_array = np.array(sentence)\n",
        "  sentence_array_norms = np.linalg.norm(sentence_array,axis=1)\n",
        "  np.linalg.norm(sentence_array/sentence_array_norms[:,None],axis=1)\n",
        "  sentence_array=sentence_array/sentence_array_norms[:,None]\n",
        "  sdots=sentence_array@sentence_array.transpose()\n",
        "  sdots=np.clip(sdots,-1.,1.)\n",
        "  pairwise_euclidean_dist=np.sqrt(2-2*sdots)\n",
        "  pairwise_geodesic_dist=np.arccos(sdots)\n",
        "  #media = np.linalg.norm(np.mean(sentence,axis=0))\n",
        "  #print(\"media\",media)\n",
        "  massimi.append(np.max(pairwise_geodesic_dist)*2/np.pi)\n",
        "  #print(\"max\",np.max(pairwise_geodesic_dist)*2/np.pi)\n",
        "plt.hist(massimi,alpha=0.5)#,ls=\"solid\",lw=3,ec=\"k\",histtype='stepfilled')\n",
        "plt.title(\"Histogram of the maximal distance between words in the sentences\",fontsize=20)\n",
        "plt.xlabel(\"max distance/(pi/2)\",fontsize=15)\n",
        "plt.ylabel(\"frequency\",fontsize=15)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "id": "aSqV5G9MYL9P",
        "outputId": "bb942709-d85b-4ec8-c400-763e88f13f01"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'frequency')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m         \u001b[0mratio\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mratio\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlogical\u001b[0m \u001b[0mto\u001b[0m \u001b[0mphysical\u001b[0m \u001b[0mpixels\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m         \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m_get_renderer\u001b[0;34m(figure, print_method)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_without_rendering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_non_interactive_terminal_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m     \"\"\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mMetadata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPNG\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0mpairs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mbytes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlatin\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mencodable\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mAccording\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPNG\u001b[0m \u001b[0mspecification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mshorter\u001b[0m \u001b[0mthan\u001b[0m \u001b[0;36m79\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mchars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_png' from 'matplotlib' (/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Refined way: as before but weighting the words for each sentence with Tf-Idf"
      ],
      "metadata": {
        "id": "QDXgmtGKYcwR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s2vTFIDF={}\n",
        "pesi_tot=computeTfIdf(data,corpus)\n",
        "num_sent=len(data)\n",
        "count=0\n",
        "for i in range(num_sent):\n",
        "  len_sent=len(data[i])\n",
        "  vettori=[]\n",
        "  k=0\n",
        "  for parola in corpus[count:count+len_sent]:\n",
        "    vett=vocGlove[parola]*pesi_tot[i][k]\n",
        "    vettori.append(vett) #/np.linalg.norm(vett)\n",
        "    k += 1\n",
        "  count += len_sent\n",
        "  s2vTFIDF[i]= np.mean(vettori,axis=0)\n"
      ],
      "metadata": {
        "id": "kxlZRyHfYfko"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other way: weighting each vector with the frequency from Github\n",
        "https://github.com/rspeer/wordfreq\n"
      ],
      "metadata": {
        "id": "nH2ERu0gYh-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s2vwordfrq={}\n",
        "\n",
        "num_sent=len(data)\n",
        "count=0\n",
        "for i in range(num_sent):\n",
        "  len_sent=len(data[i])\n",
        "  vettori=[]\n",
        "  k=0\n",
        "  for parola in corpus[count:count+len_sent]:\n",
        "    vett=vocGlove[parola]/word_frequency(parola,\"en\")\n",
        "    vettori.append(vett) #/np.linalg.norm(vett)\n",
        "    k += 1\n",
        "  count += len_sent\n",
        "  s2vwordfrq[i]= np.mean(vettori,axis=0)\n"
      ],
      "metadata": {
        "id": "6svplljAYmP2"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the nearest vector to the sentence2vec doing the projection to the vector"
      ],
      "metadata": {
        "id": "XOR2xxiZYroq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Projection(vector, num):\n",
        "  '''\n",
        "  Given the vector and a number num, the function returns the num nearest vectors to vector\n",
        "  in the sense of cosine distance\n",
        "  '''\n",
        "  vicini=[]\n",
        "  lista=[]\n",
        "  chiavi=list(word_to_index.keys())\n",
        "\n",
        "  for word in chiavi:\n",
        "    pr = np.dot(vector, vocGlove[word])/(np.linalg.norm(vector)*np.linalg.norm(vocGlove[word]))\n",
        "    lista.append(pr)\n",
        "  ordine=sorted(lista,reverse=True)\n",
        "  maxs=[]\n",
        "  for i in range(num):\n",
        "    mm=max(lista)\n",
        "    mm_index=lista.index(mm)\n",
        "    maxs.append(mm_index)\n",
        "    lista.remove(mm)\n",
        "  for i in maxs:\n",
        "    vicini.append(chiavi[i])\n",
        "  frase=\"[\"\n",
        "  for i in range(num-1):\n",
        "    frase += vicini[i].upper()+\", \"\n",
        "  frase += vicini[num-1].upper()+\"]\"\n",
        "\n",
        "  return frase  \n",
        "\n",
        "def Projection2(vector,num):\n",
        "  vicini=[]\n",
        "  lista=[]\n",
        "  chiavi=list(word_to_index.keys())\n",
        "\n",
        "  for word in chiavi:\n",
        "    pr = np.linalg.norm(vector, vocGlove[word])\n",
        "    lista.append(pr)\n",
        "  maxs=[]\n",
        "  for i in range(num):\n",
        "    mm=max(lista)\n",
        "    mm_index=lista.index(mm)\n",
        "    maxs.append(mm_index)\n",
        "    lista.remove(mm)\n",
        "  for i in maxs:\n",
        "    vicini.append(chiavi[i])\n",
        "  return vicini  "
      ],
      "metadata": {
        "id": "UrZJiF_QYwqi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Results"
      ],
      "metadata": {
        "id": "BJfYakuaZVoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Fs=10\n",
        "Ls=20\n",
        "print(\"First row: sentence\")\n",
        "print(\"3 nearest in meaning words using: \\n -1 vanilla mean \\n -2 tfidf \\n -3 frequency from github\")\n",
        "for i in range(Fs,Ls):\n",
        "  simili=Projection(sentence2vec[i],3)\n",
        "  tf=Projection(s2vTFIDF[i],3)\n",
        "  freq=Projection(s2vwordfrq[i],3)\n",
        "  print(frasi[i])\n",
        "  print(simili,\",\",tf,\",\",freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxuB-th2ZYYz",
        "outputId": "73241f63-f0d4-4bbd-80e8-6d0b32b6c896"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: sentence\n",
            "3 nearest in meaning words using: \n",
            " -1 vanilla mean \n",
            " -2 tfidf \n",
            " -3 frequency from github\n",
            "In retrospect, we should have taken to the streets when McConnell refused to let Obama rightfully fill a Supreme Court vacancy. That was an epic fail on our part as citizens.\n",
            "[QUESTION, BOOMERS, CAN] , [QUESTION, BOOMERS, CAN] , [RIGHTFULLY, RETROSPECT, FUCKING]\n",
            "Hey, you misguided nincompoops. Here's a glimpse of #FairandBalanced response to a hatemonger https://t.co/inPIz0twO6.\n",
            "[STUPID, MISGUIDED, LAND] , [MISGUIDED, INNER, LAND] , [HATEMONGER, NINCOMPOOPS, TIME]\n",
            "Social Security and Medicare are not \"Entitlements\" that can be renegotiated.   The term is \"DEFERRED PAY.\"  I've paid into the system my entire life, and so have you. Anyone stealing your deferred pay needs to be voted out.\n",
            "[PAY, WIFE, CHARITABLE] , [PAY, WIFE, PAY] , [RENEGOTIATED, TERM, CHRIS]\n",
            "'You let a weird guy whisper in your ear and you didn't trust your daughter.' --my 7-year-old girl https://t.co/Iqhx8PUGSo\n",
            "[LOVE, POOR, SCARIEST] , [LOVE, KNOWING, POOR] , [WHISPER, PRETEND, WHISPER]\n",
            "@saladinahmed May the initiative to require neutral redistricting in Michigan succeed.   For those unaware of it: https://t.co/DfBydPOnIH.\n",
            "[CHANGE, BOOMERS, INVITED] , [CHANGE, BOOMERS, INVITED] , [REDISTRICTING, SUBSTANCE, MULTIPLE]\n",
            "it's interesting watching this with them because I thought Gollum might scare them but they mostly just get upset when he's mistreated.\n",
            "[AFRAID, SCARIEST, APOCALYPTIC] , [AFRAID, SCARIEST, ENTIRE] , [GOLLUM, UPSET, NICE]\n",
            "I have but one request this Christmas, guys. Take the #WarOnChristmas hashtag and fill it with accounts of the horrors of battling elves across snowy fields and firing anti-aircraft missiles at sleds streaking overhead.\n",
            "[OUT, HEMOPHILIA, STATE] , [CARRY, OUT, STATE] , [ANTIAIRCRAFT, ANTIAIRCRAFT, ANTIAIRCRAFT]\n",
            "It is fascinating to see female Democratic senators (with the help of male colleagues) take this coordinated action against Al Franken.\n",
            "[BOTH, MODEL, WORDS] , [BOTH, MODEL, WORDS] , [FRANKEN, TIPPER, MCCONNELL]\n",
            "marvel should make a mockumentary about loki as odin and the behind the scenes drama that went on while loki was trying to produce and direct his play. i wanna see tantrums about sets, actors and interviews with asgardians who are ya we know it's loki\n",
            "[CHARACTER, MOVIE, IMPORTANTLY] , [LOKI, CRAZIEST, LAYING] , [ASGARDIANS, LOKI, ARAGORN]\n",
            "It seems like a whole bunch of narcissistic sexually harassing dudes who have never had to notice all the people around them who cleaned up their messes are suddenly being handed mops.  I'm all for mailing a few mops to the White House, personally.\n",
            "[THEM, ENJOY, TALKING] , [HANDS, THEM, FOUNDERS] , [MOPS, MESSES, SMELLY]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-mean clustering"
      ],
      "metadata": {
        "id": "dG3B9c4faSE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "for i in range(Fs,Ls):\n",
        "  X = sentencesAsVec[i]\n",
        "  kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
        "  centers=np.transpose(kmeans.cluster_centers_)\n",
        "  print(frasi[i])\n",
        "  p1=Projection(centers[:,0],1)[:-1]\n",
        "  p2=Projection(centers[:,1],1)[1:-1]\n",
        "  p3=Projection(centers[:,2],1)[1:]\n",
        "  print(p1,p2,p3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdp6BLkXaUzI",
        "outputId": "dd7c9ad3-d1eb-41c1-c6ac-c91c8aac8782"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In retrospect, we should have taken to the streets when McConnell refused to let Obama rightfully fill a Supreme Court vacancy. That was an epic fail on our part as citizens.\n",
            "[REFUSED VACANCY RETROSPECT]\n",
            "Hey, you misguided nincompoops. Here's a glimpse of #FairandBalanced response to a hatemonger https://t.co/inPIz0twO6.\n",
            "[RESPONSE HEY NINCOMPOOPS]\n",
            "Social Security and Medicare are not \"Entitlements\" that can be renegotiated.   The term is \"DEFERRED PAY.\"  I've paid into the system my entire life, and so have you. Anyone stealing your deferred pay needs to be voted out.\n",
            "[PAY MAKING DEFERRED]\n",
            "'You let a weird guy whisper in your ear and you didn't trust your daughter.' --my 7-year-old girl https://t.co/Iqhx8PUGSo\n",
            "[MOTHER BIT TRUST]\n",
            "@saladinahmed May the initiative to require neutral redistricting in Michigan succeed.   For those unaware of it: https://t.co/DfBydPOnIH.\n",
            "[SHOULD MICHIGAN REDISTRICTING]\n",
            "it's interesting watching this with them because I thought Gollum might scare them but they mostly just get upset when he's mistreated.\n",
            "[REALLY MISTREATED GOLLUM]\n",
            "I have but one request this Christmas, guys. Take the #WarOnChristmas hashtag and fill it with accounts of the horrors of battling elves across snowy fields and firing anti-aircraft missiles at sleds streaking overhead.\n",
            "[COME SLEDS MISSILES]\n",
            "It is fascinating to see female Democratic senators (with the help of male colleagues) take this coordinated action against Al Franken.\n",
            "[WAYS GOP FEMALE]\n",
            "marvel should make a mockumentary about loki as odin and the behind the scenes drama that went on while loki was trying to produce and direct his play. i wanna see tantrums about sets, actors and interviews with asgardians who are ya we know it's loki\n",
            "[LOKI MAKING DRAMA]\n",
            "It seems like a whole bunch of narcissistic sexually harassing dudes who have never had to notice all the people around them who cleaned up their messes are suddenly being handed mops.  I'm all for mailing a few mops to the White House, personally.\n",
            "[SEXUALLY TAKEN MESSES]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# k-mean clustering on the sphere"
      ],
      "metadata": {
        "id": "bz1mb3NJbCoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "!pip install geomstats\n",
        "\n",
        "sys.path.append(os.path.dirname(os.getcwd()))\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import logging\n",
        "import math\n",
        "from itertools import product\n",
        "from scipy.stats import beta\n",
        "import geomstats.algebra_utils as utils\n",
        "import geomstats.backend as gs\n",
        "from geomstats.geometry.base import LevelSet\n",
        "from geomstats.geometry.euclidean import Euclidean, EuclideanMetric\n",
        "from geomstats.geometry.riemannian_metric import RiemannianMetric\n",
        "from geomstats.learning.kmeans import RiemannianKMeans\n",
        "from geomstats.geometry.hypersphere import Hypersphere\n",
        "\n",
        "manifold = Hypersphere(dim=49)\n",
        "metric = manifold.metric\n"
      ],
      "metadata": {
        "id": "l1CRyYXCi0wC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b79162-f200-4133-fb1c-69f91cf99c9d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: geomstats in /usr/local/lib/python3.7/dist-packages (2.5.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from geomstats) (1.0.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.7/dist-packages (from geomstats) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.7/dist-packages (from geomstats) (3.5.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from geomstats) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.7/dist-packages (from geomstats) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from geomstats) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (2.8.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (4.37.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (7.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.4->geomstats) (21.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.3.4->geomstats) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.5->geomstats) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->geomstats) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->geomstats) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ProjectionG(vector):\n",
        "  vicini=[]\n",
        "  lista=[]\n",
        "  chiavi=list(word_to_index.keys())\n",
        "  n=np.linalg.norm(vector)\n",
        "  vector=vector/np.sqrt(n**2+1.e-6)\n",
        "\n",
        "  for word in chiavi:\n",
        "    v2=vocGlove[word]/np.linalg.norm(vocGlove[word])\n",
        "    pr = np.math.acos( np.dot(vector, v2))\n",
        "    lista.append(pr)\n",
        "  mm=min(lista)\n",
        "  mm_index=lista.index(mm)\n",
        "  vicino=chiavi[mm_index]\n",
        "  return vicino.upper()"
      ],
      "metadata": {
        "id": "p0z0tQkgxK9i"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"k-mean clustering geodesic\")\n",
        "for i in range(10,20):#Fs,Ls):\n",
        "  data = np.array(sentencesAsVec[i])\n",
        "  kmeans = RiemannianKMeans(metric, 3, tol=1e-3, init_step_size=1.0)\n",
        "  kmeans.fit(data)\n",
        "  labels = kmeans.predict(data)\n",
        "  centroids = np.transpose(kmeans.centroids)\n",
        "  print(frasi[i])\n",
        "  c1=centroids[:,0]/np.linalg.norm(centroids[:,0])\n",
        "  c2=centroids[:,1]/np.linalg.norm(centroids[:,1])\n",
        "  c3=centroids[:,2]/np.linalg.norm(centroids[:,2])\n",
        "  print(\"[\",ProjectionG(c1),ProjectionG(c2),ProjectionG(c3),\"]\")"
      ],
      "metadata": {
        "id": "JxwYysVgj5tC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e9e408-3d3d-47f8-81e3-25593fa4c2ab"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k-mean clustering geodesic\n",
            "In retrospect, we should have taken to the streets when McConnell refused to let Obama rightfully fill a Supreme Court vacancy. That was an epic fail on our part as citizens.\n",
            "[ STREETS SHOULD EPIC ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey, you misguided nincompoops. Here's a glimpse of #FairandBalanced response to a hatemonger https://t.co/inPIz0twO6.\n",
            "[ HERES MOMENT MISGUIDED ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Social Security and Medicare are not \"Entitlements\" that can be renegotiated.   The term is \"DEFERRED PAY.\"  I've paid into the system my entire life, and so have you. Anyone stealing your deferred pay needs to be voted out.\n",
            "[ NEEDS PAY STEALING ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'You let a weird guy whisper in your ear and you didn't trust your daughter.' --my 7-year-old girl https://t.co/Iqhx8PUGSo\n",
            "[ WHISPER FATHER GIRL ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@saladinahmed May the initiative to require neutral redistricting in Michigan succeed.   For those unaware of it: https://t.co/DfBydPOnIH.\n",
            "[ SHOULD REQUIRE REDISTRICTING ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it's interesting watching this with them because I thought Gollum might scare them but they mostly just get upset when he's mistreated.\n",
            "[ MISTREATED INTERESTING WATCHING ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have but one request this Christmas, guys. Take the #WarOnChristmas hashtag and fill it with accounts of the horrors of battling elves across snowy fields and firing anti-aircraft missiles at sleds streaking overhead.\n",
            "[ TAKEN HASHTAG PULLING ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It is fascinating to see female Democratic senators (with the help of male colleagues) take this coordinated action against Al Franken.\n",
            "[ BOTH DEMOCRATIC FASCINATING ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "marvel should make a mockumentary about loki as odin and the behind the scenes drama that went on while loki was trying to produce and direct his play. i wanna see tantrums about sets, actors and interviews with asgardians who are ya we know it's loki\n",
            "[ LOKI DRAMA WELL ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n",
            "WARNING:root:Maximum number of iterations 100 reached. The mean may be inaccurate\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It seems like a whole bunch of narcissistic sexually harassing dudes who have never had to notice all the people around them who cleaned up their messes are suddenly being handed mops.  I'm all for mailing a few mops to the White House, personally.\n",
            "[ MOPS TAKEN SEXUALLY ]\n"
          ]
        }
      ]
    }
  ]
}