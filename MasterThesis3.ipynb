{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MasterThesis3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMnfi/UZWDNf8sKJjVdyxX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ceciliasecchi/MasterThesis/blob/main/MasterThesis3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordfreq\n",
        "from wordfreq import word_frequency\n",
        "from nltk.collections import OrderedDict\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re  # For preprocessing\n",
        "import string\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import csv\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "from gensim.parsing.preprocessing import remove_stopwords,preprocess_string,preprocess_documents\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import collections\n",
        "import math\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import DefaultDict\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import bernoulli \n",
        "from scipy.special import softmax as scipy_softmax\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmFeZAMxZjKu",
        "outputId": "ba7d4ebd-53e8-44fe-8eb2-9544fbfc3519"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wordfreq\n",
            "  Downloading wordfreq-3.0.1-py3-none-any.whl (56.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 56.8 MB 18 kB/s \n",
            "\u001b[?25hCollecting ftfy>=6.1\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (1.0.4)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (3.3.0)\n",
            "Requirement already satisfied: regex>=2020.04.04 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (2022.6.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy>=6.1->wordfreq) (0.2.5)\n",
            "Installing collected packages: ftfy, wordfreq\n",
            "Successfully installed ftfy-6.1.1 wordfreq-3.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reads ‘alice.txt’ file\n",
        "sample = open(\"alice_firstpage.txt\", \"r\")\n",
        "s = sample.read()\n",
        "stopp = {'and','but','this','said','you','the','how','for','like','thats',\n",
        "         'like','thought','again','but','dont','that','ive','didnt','they','she',\n",
        "         'while','instead','would','thats','anything','its','hasnt','still',\n",
        "         'gets','went',\"wouldnt\",\"get\",\"let\",'way',\"these\",\"those\",\"havent\"\n",
        "         'looked','came','not','got','then','ive',\"were\",'there','wont'}\n",
        "voc = {}\n",
        "vocGlove = OrderedDict()\n",
        "vocGloveReverse = {}\n",
        "with open('vocGlove_Alice.csv', mode='r') as inp:\n",
        "    reader = csv.reader(inp)\n",
        "    for row in reader:\n",
        "      voc = {rows[0]:rows[1].replace(\"\\n\", \" \").replace(\"[\",\" \").replace(\"]\",\" \") for rows in reader}\n",
        "    \n",
        "for key,value in voc.items():\n",
        "  value=value.split()\n",
        "  if len(value)!=50:\n",
        "    print(key)\n",
        "  if key not in stopp:\n",
        "    vocGlove[key]=np.array(list(map(float,value)))\n",
        "\n",
        "# normalize the vectors\n",
        "for k,v in vocGlove.items():\n",
        "  v = v/np.linalg.norm(v)\n",
        "  vocGlove[k] = v\n",
        "  vocGloveReverse[str(v)]=k\n"
      ],
      "metadata": {
        "id": "29M8IOGeZnGl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dizionario=defaultdict(int)\n",
        "# Replaces escape character with space\n",
        "g = s.replace(\"\\n\",\" \")\n",
        "g = g.replace(\"’\",\" \")\n",
        "g = g.replace(\"“\", \".\")\n",
        "g = g.replace(\"”\",\".\")\n",
        "g = g.replace(\";\",\".\")\n",
        "\n",
        "f = g.split(\".\")\n",
        "#re.split('.| ;| “| ”',g)\n",
        "data = []\n",
        "# iterate through each sentence in the file\n",
        "frasi=[]\n",
        "for i in f:\n",
        "  copia=i\n",
        "  i = remove_stopwords(i)\n",
        "  i = i.translate(i.maketrans('', '', string.punctuation+\"\\—\\-\\\"\\'\\“\\”\\’\\‘\"))\n",
        "  temp = []\n",
        "\t\n",
        "\t# tokenize the sentence into words\n",
        "  for j in word_tokenize(i):\n",
        "    j = j.lower()\n",
        "    if j in vocGlove.keys() and len(j)>2:\n",
        "      dizionario[j] += 1\n",
        "      temp.append(j)\n",
        "    else: pass\n",
        "  if len(temp)>5:\n",
        "    data.append(temp)\n",
        "    frasi.append(copia)\n",
        "\n",
        "def generate_dictionary_data(text):\n",
        "    word_to_index= OrderedDict()\n",
        "    index_to_word = OrderedDict()\n",
        "    freq_dict = DefaultDict(int)\n",
        "    corpus = []\n",
        "    corpus_sent = []\n",
        "    count = 0\n",
        "    vocab_size = 0\n",
        "    \n",
        "    for row in text:\n",
        "        for word in row:\n",
        "            freq_dict[word] += 1\n",
        "            word = word.lower()\n",
        "            corpus.append(word)\n",
        "            if word_to_index.get(word) == None:\n",
        "                word_to_index.update ( {word : count})\n",
        "                index_to_word.update ( {count : word })\n",
        "                count  += 1\n",
        "    vocab_size = len(word_to_index)\n",
        "    length_of_corpus = len(corpus)\n",
        "    \n",
        "    return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus,freq_dict"
      ],
      "metadata": {
        "id": "uz21QYyeZ8bE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index,index_to_word,corpus,vocab_size,length_of_corpus,freq_dict=generate_dictionary_data(data)"
      ],
      "metadata": {
        "id": "heBa81L2aHdc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentencesAsVec=[]\n",
        "num_sent=len(data)\n",
        "\n",
        "count=0\n",
        "conteggio={}\n",
        "for i in range(num_sent):\n",
        "  len_sent=len(data[i])\n",
        "  vettori=[]\n",
        "  for parola in corpus[count:count+len_sent]:\n",
        "    vettori.append(vocGlove[parola])\n",
        "  conteggio[i]=[count,count+len_sent]\n",
        "  count += len_sent\n",
        "  sentencesAsVec.append(vettori)\n"
      ],
      "metadata": {
        "id": "riCiOn7TlG2p"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel Distance"
      ],
      "metadata": {
        "id": "iNqQRHlyju5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K=3 #number of Dirac masses in the compressed distribution\n",
        "N=50 #ambient dimension\n",
        "J=1000 #how many samples are included: more the better, otherwise there are oscillations in the SGD/Adam convergence\n",
        "quant=tf.Variable(np.random.rand(N,K),dtype=tf.float32) #this will be the set of quantization variables\n"
      ],
      "metadata": {
        "id": "_GzyNtfRj8Nk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def empirical_sampling_from_sentence(sentence, S=30):\n",
        "    ''' sample from the sentence (that is a vector of words of length 50) 30 words\n",
        "        note: sentence is a vector of vectors, then the output is a np.array \n",
        "              where each coloumn corresponds to a word\n",
        "    '''\n",
        "    N=len(sentence)\n",
        "    Y=[]\n",
        "    if N>=S:\n",
        "        indices=np.random.choice(N, S)\n",
        "    else:\n",
        "        indices=np.concatenate((np.arange(N),np.random.choice(N,S-N)),axis=0)\n",
        "        #Y=sentence\n",
        "    for i in indices:\n",
        "        Y.append(sentence[i])\n",
        "    Y=np.transpose(np.asarray(Y))\n",
        "    return Y\n",
        "    #return Y,indices\n",
        "\n",
        "empirical_sampling=empirical_sampling_from_sentence\n",
        "\n",
        "def empirical_radon_sobolev_distance_sq(X,Y,local_alphas=None,betas=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2D NxK matrix\n",
        "        input data sample, each column a vector of dimension N, notation X_k\n",
        "    Y : same as X for the second distribution\n",
        "    alphas : 1D array of weights for X, in my case alphas==1/K\n",
        "    betas : same as alphas for Y , in my case betas==1/30?\n",
        "\n",
        "    Note: X,alphas are Tensor/Variable while Y/betas are numpy array\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Radon-Sobolev distance\n",
        "    \n",
        "    '''\n",
        "    N,K=X.shape\n",
        "    Ny,J=Y.shape\n",
        "    Ytensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "\n",
        "    # this is my case.. maybe in the future put tf-idf as weights?\n",
        "    if local_alphas is None:\n",
        "      alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "    else:\n",
        "      alphas=tf.nn.softmax(local_alphas)\n",
        "    if betas is None:\n",
        "        betas = np.ones(J)/J\n",
        "    betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "\n",
        "    # serie di check per vedere che gli input abbiano senso in dimensione\n",
        "    # tf.rank = # indeces required to uniquely select an element\n",
        "    assert (tf.rank(X).numpy()==2) & (Y.ndim==2) & (tf.rank(alphas).numpy()==1) & (betas.ndim==1),\"invalid input dimensions\"\n",
        "    Ka,=alphas.shape\n",
        "    Jb,=betas.shape\n",
        "    assert (N==Ny) & (K==Ka) & (J==Jb), 'invalid input dimensions'\n",
        "\n",
        "    #construct big matrix: [X,Y]=[X_1,...,X_3,Y_1,...,Y_30]\n",
        "    points=tf.concat([X,Ytensor],axis=1)\n",
        "    # construct big vector of weights \n",
        "    gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "\n",
        "    #construct the distances matrix as a K+J x K+J matrix\n",
        "    distZZ = tf.math.sqrt(1.0e-10+tf.math.reduce_sum(tf.square(tf.expand_dims(points,2)-tf.expand_dims(points,1)),axis=0))-1.0e-5\n",
        "    # Given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed.\n",
        "    # I return thee matrix multiplied by the vectors\n",
        "    return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))\n",
        "\n",
        "def empirical_radon_sobolev_distance_sq_geod(X,Y,local_alphas=None,betas=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2D NxK matrix\n",
        "        input data sample, each column a vector of dimension N, notation X_k\n",
        "    Y : same as X for the second distribution\n",
        "    alphas : 1D array of weights for X, in my case alphas==1/K\n",
        "    betas : same as alphas for Y , in my case betas==1/30?\n",
        "\n",
        "    Note: X,alphas are Tensor/Variable while Y/betas are numpy array\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Radon-Sobolev distance\n",
        "    \n",
        "    '''\n",
        "    N,K=X.shape\n",
        "    Ny,J=Y.shape\n",
        "    Ytensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "\n",
        "    # rinormalizzo ogni volta\n",
        "    norma=tf.norm(X,axis=0)\n",
        "    X=X/tf.sqrt(norma**2+1.e-6)\n",
        "\n",
        "\n",
        "    # this is my case.. maybe in the future put tf-idf as weights?\n",
        "    if local_alphas is None:\n",
        "      alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "    else:\n",
        "      alphas=tf.nn.softmax(local_alphas)\n",
        "    if betas is None:\n",
        "        betas = np.ones(J)/J\n",
        "    betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "\n",
        "    # serie di check per vedere che gli input abbiano senso in dimensione\n",
        "    # tf.rank = # indeces required to uniquely select an element\n",
        "    assert (tf.rank(X).numpy()==2) & (Y.ndim==2) & (tf.rank(alphas).numpy()==1) & (betas.ndim==1),\"invalid input dimensions\"\n",
        "    Ka,=alphas.shape\n",
        "    Jb,=betas.shape\n",
        "    assert (N==Ny)& (K==Ka)&(J==Jb), 'invalid input dimensions'\n",
        "\n",
        "\n",
        "    #construct big matrix: [X,Y]=[X_1,...,X_3,Y_1,...,Y_30]\n",
        "    #points=np.transpose(tf.concat([X,Ytensor],axis=1))\n",
        "    points=tf.concat([X,Ytensor],axis=1)\n",
        "\n",
        "    # construct big vector of weights \n",
        "    gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "\n",
        "    #construct the distances matrix as a K+J x K+J matrix\n",
        "    \n",
        "    distZZ = tf.math.acos(tf.matmul(tf.transpose(points),points)/1.0001)\n",
        "    # Given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed.\n",
        "    # I return thee matrix multiplied by the vectors\n",
        "    return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))#-sum(np.multiply(Lag,np.power(np.linalg.norm(X,axis=0),2)))+K\n",
        "\n",
        "                            "
      ],
      "metadata": {
        "id": "7ZUScc6_j9xC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distanzaH(sentence,compressed):\n",
        "        J=len(sentence)\n",
        "\n",
        "        sentence=np.transpose(sentence)\n",
        "        N,K=compressed.shape\n",
        "        Ytensor = tf.convert_to_tensor(sentence, dtype=tf.float32)\n",
        "        alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "        betas = np.ones(J)/J\n",
        "        betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "        points=tf.concat([compressed,sentence],axis=1)\n",
        "        gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "        distZZ = tf.math.sqrt(1.0e-10+tf.math.reduce_sum(tf.square(tf.expand_dims(points,2)-tf.expand_dims(points,1)),axis=0))-1.0e-5\n",
        "        return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))\n",
        "\n",
        "def distanzaGeo(sentence,compressed):\n",
        "        J=len(sentence)\n",
        "        norma=tf.norm(compressed,axis=0)\n",
        "        compressed=compressed/tf.sqrt(norma**2+1.e-6)\n",
        "        sentence=np.transpose(sentence)\n",
        "        N,K=compressed.shape\n",
        "        Ytensor = tf.convert_to_tensor(sentence, dtype=tf.float32)\n",
        "        alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "        betas = np.ones(J)/J\n",
        "        betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "        points=tf.concat([compressed,sentence],axis=1)\n",
        "        gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "        distZZ = tf.math.acos(tf.matmul(tf.transpose(points),points)/1.0001)\n",
        "        return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))\n",
        "\n"
      ],
      "metadata": {
        "id": "UtSY0V-2kkvA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find one word at a time \n",
        "Using as kernel:\n",
        "$h(x,y)=\\sqrt{a^2+|x-y|^2}-a$"
      ],
      "metadata": {
        "id": "K9RGqdZykRM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  num_sent=len(data)\n",
        "  step=6\n",
        "  for w in range(num_sent):\n",
        "    sentence=sentencesAsVec[w]\n",
        "    K=3 #number of Dirac masses in the compressed distribution\n",
        "    N=50 #ambient dimension\n",
        "    J=1000 #how many samples are included: more the better, otherwise there are oscillations in the SGD/Adam convergence\n",
        "    quant=tf.Variable(np.random.rand(N,K),dtype=tf.float32) #this will be the set of quantization variables\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.1,beta_1=0.9, beta_2=0.999)# TODO: use lr_schedule for step decay schedule !\n",
        "    loss_quant = lambda: empirical_radon_sobolev_distance_sq(quant,empirical_sampling(sentence))\n",
        "\n",
        "    nr_iter=100\n",
        "    loss_val=[]\n",
        "    opt_val=[]\n",
        "\n",
        "    for sc in range(nr_iter):\n",
        "      step_count = opt.minimize(loss_quant, [quant]).numpy()\n",
        "      opt_val.append(quant.numpy())\n",
        "      loss_val.append(loss_quant())\n",
        "    c1=quant[:,0]\n",
        "    c2=quant[:,1]\n",
        "    c3=quant[:,2]\n",
        "    compression=[c1,c2,c3]\n",
        "    cdcd=tf.Variable(np.array(compression).reshape(50,-1), dtype=tf.float32)\n",
        "    dc=math.sqrt(distanzaH(sentence,cdcd).numpy())\n",
        "    print(frasi[w])\n",
        "    N=len(sentence)\n",
        "    #indici=random.sample(range(N), 3)\n",
        "    score1={}\n",
        "    for parola in vocGlove.keys():\n",
        "      parola_vec=vocGlove[parola]\n",
        "      compressed=tf.Variable(np.array([parola_vec]).reshape(50,-1), dtype=tf.float32)\n",
        "\n",
        "      score1[parola]=math.sqrt(distanzaH(compression,compressed).numpy())\n",
        "    best1=min(score1.values())\n",
        "    migliore1=[key for key, value in score1.items() if value == best1]\n",
        "    parola1=vocGlove[migliore1[0]]\n",
        "    score2={}\n",
        "    for parola in vocGlove.keys():\n",
        "      if parola != migliore1:\n",
        "        parola_vec=vocGlove[parola]\n",
        "        compressed=tf.Variable(np.array([parola1,parola_vec]).reshape(50,-1), dtype=tf.float32)\n",
        "        score2[parola]=math.sqrt(distanzaH(compression,compressed).numpy())\n",
        "    best2=min(score2.values())\n",
        "    migliore2=[key for key, value in score2.items() if value == best2]\n",
        "    parola2=vocGlove[migliore2[0]]\n",
        "    score3={}\n",
        "    for parola in vocGlove.keys():\n",
        "      if parola != migliore1 and parola!=migliore2:\n",
        "        parola_vec=vocGlove[parola]\n",
        "        compressed=tf.Variable(np.array([parola1,parola2,parola_vec]).reshape(50,-1), dtype=tf.float32)\n",
        "        score3[parola]=math.sqrt(distanzaH(compression,compressed).numpy())\n",
        "    best3=min(score3.values())\n",
        "    migliore3=[key for key, value in score3.items() if value == best3]\n",
        "    print(\"[\",migliore1[0].upper(),\", \",migliore2[0].upper(),\", \",migliore3[0].upper(),\"]\",\"d(S,C1)=\",round(dc,4),\"d(S,CP)=\",round(best3,4))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_-cdbuNkft3",
        "outputId": "f1ae4f4b-5d22-40aa-99e4-218e031995bf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, \n",
            "[ SISTER ,  DISOBEY ,  SONG ] d(S,C1)= 0.6234 d(S,CP)= 0.6831\n",
            "  So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her\n",
            "[ SO ,  DETACH ,  EXCLAMATION ] d(S,C1)= 0.6504 d(S,CP)= 0.7101\n",
            " nor did Alice think it so _very_ much out of the way to hear the Rabbit say to itself, \n",
            "[ HEAR ,  DISOBEY ,  VERSION ] d(S,C1)= 0.6442 d(S,CP)= 0.6888\n",
            " (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural)\n",
            "[ WONDERED ,  DISOBEY ,  FARMER ] d(S,C1)= 0.7031 d(S,CP)= 0.6481\n",
            " but when the Rabbit actually _took a watch out of its waistcoat-pocket_, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge\n",
            "[ SO ,  DISOBEY ,  TOTO ] d(S,C1)= 0.6125 d(S,CP)= 0.6675\n",
            "  The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well\n",
            "[ SUDDENLY ,  DETACH ,  FIG ] d(S,C1)= 0.6933 d(S,CP)= 0.7257\n",
            "  Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next\n",
            "[ GOING ,  DISOBEY ,  PASS ] d(S,C1)= 0.7542 d(S,CP)= 0.7226\n",
            " then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves\n",
            "[ FILLED ,  KINDLY ,  STRINGS ] d(S,C1)= 0.6714 d(S,CP)= 0.6646\n",
            ", but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody underneath, so managed to put it into one of the cupboards as she fell past it\n",
            "[ AWAY ,  DETACH ,  HELL ] d(S,C1)= 0.6083 d(S,CP)= 0.6655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using as kernel:\n",
        "$h(x,y)=\\arccos{x\\cdot y}$"
      ],
      "metadata": {
        "id": "bNTRDIO0nHU2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  K=3 #number of Dirac masses in the compressed distribution\n",
        "  N=50 #ambient dimension\n",
        "  J=1000 #how many samples are included: more the better, otherwise there are oscillations in the SGD/Adam convergence\n",
        "  \n",
        "  for w in range(num_sent):\n",
        "    random.seed(10)\n",
        "    sentence=sentencesAsVec[w]\n",
        "    num_rand=np.random.rand(N,K)\n",
        "    den_rand=np.tile(np.linalg.norm(num_rand,axis=0),(N,1))\n",
        "    quant=tf.Variable(num_rand/den_rand,dtype=tf.float32) #this will be the set of quantization variables\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.01,beta_1=0.9, beta_2=0.999)# TODO: use lr_schedule for step decay schedule !\n",
        "    Y=empirical_sampling(sentence)\n",
        "    #Y,indices=empirical_sampling(sentence)\n",
        "    loss_quant = lambda: empirical_radon_sobolev_distance_sq_geod(quant,Y)\n",
        "\n",
        "    nr_iter=200\n",
        "    loss_val=[]\n",
        "    opt_val=[]\n",
        "\n",
        "    for sc in range(nr_iter):\n",
        "      vq=quant.numpy()\n",
        "      quant.assign(vq/np.linalg.norm(vq,axis=0))\n",
        "      step_count = opt.minimize(loss_quant, [quant]).numpy()\n",
        "      opt_val.append(quant.numpy())\n",
        "      loss_val.append(loss_quant())\n",
        "    c1=quant[:,0]/np.linalg.norm(quant[:,0])\n",
        "    c2=quant[:,1]/np.linalg.norm(quant[:,1])\n",
        "    c3=quant[:,2]/np.linalg.norm(quant[:,2])\n",
        "    compression=[c1,c2,c3]\n",
        "\n",
        "    cdcd=tf.Variable(np.array(compression).reshape(50,-1), dtype=tf.float32)\n",
        "    dc=math.sqrt(distanzaGeo(sentence,cdcd).numpy())\n",
        "    print(frasi[w])\n",
        "    #indici=random.sample(range(N), 3)\n",
        "    score1={}\n",
        "    for parola in vocGlove.keys():\n",
        "      parola_vec=vocGlove[parola]\n",
        "      compressed=tf.Variable(np.array([parola_vec]).reshape(50,-1), dtype=tf.float32)/np.linalg.norm(parola_vec)\n",
        "      score1[parola]=math.sqrt(distanzaGeo(compression,compressed).numpy())\n",
        "    #print(score1)\n",
        "    best1=min(score1.values())\n",
        "    migliore1=[key for key, value in score1.items() if value == best1]\n",
        "    parola1=vocGlove[migliore1[0]]/np.linalg.norm(vocGlove[migliore1[0]])\n",
        "    score2={}\n",
        "    for parola in vocGlove.keys():\n",
        "      if parola != migliore1:\n",
        "        parola_vec=vocGlove[parola]/np.linalg.norm(vocGlove[parola])\n",
        "\n",
        "        compressed=tf.Variable(np.array([parola1,parola_vec]).reshape(50,-1), dtype=tf.float32)\n",
        "        score2[parola]=math.sqrt(distanzaGeo(compression,compressed).numpy())\n",
        "    best2=min(score2.values())\n",
        "    migliore2=[key for key, value in score2.items() if value == best2]\n",
        "    parola2=vocGlove[migliore2[0]]\n",
        "    score3={}\n",
        "    for parola in vocGlove.keys():\n",
        "      if parola != migliore1 and parola!=migliore2:\n",
        "        parola_vec=vocGlove[parola]/np.linalg.norm(vocGlove[parola])\n",
        "        compressed=tf.Variable(np.array([parola1,parola2,parola_vec]).reshape(50,-1), dtype=tf.float32)\n",
        "        score3[parola]=math.sqrt(distanzaGeo(compression,compressed).numpy())\n",
        "    best3=min(score3.values())\n",
        "    migliore3=[key for key, value in score3.items() if value == best3]\n",
        "    print(\"[\",migliore1[0].upper(),\", \",migliore2[0].upper(),\", \",migliore3[0].upper(),\"]\",\"d(S,C1)=\",round(dc,4),\"d(S,CP)=\",round(best3,4))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-9cYl6GkwYa",
        "outputId": "3ea88c14-52c2-4efa-9276-78c46e32911d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, \n",
            "[ HER ,  DISOBEY ,  LORY ] d(S,C1)= 0.6587 d(S,CP)= 0.6744\n",
            "  So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her\n",
            "[ GETTING ,  DETACH ,  EXCLAMATION ] d(S,C1)= 0.7013 d(S,CP)= 0.7709\n",
            " nor did Alice think it so _very_ much out of the way to hear the Rabbit say to itself, \n",
            "[ THINK ,  DISOBEY ,  LORY ] d(S,C1)= 0.726 d(S,CP)= 0.7135\n",
            " (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural)\n",
            "[ TIME ,  DISOBEY ,  FARMER ] d(S,C1)= 0.7249 d(S,CP)= 0.7448\n",
            " but when the Rabbit actually _took a watch out of its waistcoat-pocket_, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge\n",
            "[ TIME ,  DISOBEY ,  KETTLE ] d(S,C1)= 0.6584 d(S,CP)= 0.6811\n",
            "  The rabbit-hole went straight on like a tunnel for some way, and then dipped suddenly down, so suddenly that Alice had not a moment to think about stopping herself before she found herself falling down a very deep well\n",
            "[ SUDDENLY ,  DETACH ,  ROSES ] d(S,C1)= 0.7517 d(S,CP)= 0.7782\n",
            "  Either the well was very deep, or she fell very slowly, for she had plenty of time as she went down to look about her and to wonder what was going to happen next\n",
            "[ EVEN ,  DETACH ,  NOTICED ] d(S,C1)= 0.8123 d(S,CP)= 0.7602\n",
            " then she looked at the sides of the well, and noticed that they were filled with cupboards and book-shelves\n",
            "[ FILLED ,  STIGAND ,  QUEEN ] d(S,C1)= 0.6923 d(S,CP)= 0.7119\n",
            ", but to her great disappointment it was empty: she did not like to drop the jar for fear of killing somebody underneath, so managed to put it into one of the cupboards as she fell past it\n",
            "[ AWAY ,  DETACH ,  HELL ] d(S,C1)= 0.6944 d(S,CP)= 0.6586\n"
          ]
        }
      ]
    }
  ]
}