{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MasterThesis2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnZWqyMU4aRsWeU1o+6PYo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ceciliasecchi/MasterThesis/blob/main/MasterThesis2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osN7yftnjqAY",
        "outputId": "6b3db339-ff75-4b16-f40f-ed1cb319745d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.7/dist-packages (3.0.1)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (1.0.4)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (6.1.1)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (3.3.0)\n",
            "Requirement already satisfied: regex>=2020.04.04 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (2022.6.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy>=6.1->wordfreq) (0.2.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install wordfreq\n",
        "from wordfreq import word_frequency\n",
        "from nltk.collections import OrderedDict\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re  # For preprocessing\n",
        "import string\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import csv\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "from gensim.parsing.preprocessing import remove_stopwords,preprocess_string,preprocess_documents\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import collections\n",
        "import math\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import bernoulli \n",
        "from scipy.special import softmax as scipy_softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopp = {'and','but','this','said','you','the','how','for','like','thats','hes',\n",
        "         'like','thought','again','but','dont','that','ive','didnt','they','she',\n",
        "         'while','instead','would','thats','anything','its','hasnt','still',\n",
        "         'gets','went',\"wouldnt\",\"get\",\"let\",'way',\"these\",\"those\",\"havent\"\n",
        "         'looked','came','not','got','then','ive',\"were\",'there','wont'}\n",
        "\n",
        "# Reads 'tweets.txt’ file\n",
        "sample = open(\"tweets.txt\", \"r\")\n",
        "s = sample.read()\n",
        "\n",
        "voc = {}\n",
        "vocGlove = OrderedDict()\n",
        "vocGloveReverse = {}\n",
        "with open('vocGlove_tweets.csv', mode='r') as inp:\n",
        "    reader = csv.reader(inp)\n",
        "    for row in reader:\n",
        "      voc = {rows[0]:rows[1].replace(\"\\n\", \" \").replace(\"[\",\" \").replace(\"]\",\" \") for rows in reader}\n",
        "    \n",
        "for key,value in voc.items():\n",
        "  value=value.split()\n",
        "  if len(value)!=50:\n",
        "    print(key)\n",
        "  if key not in stopp:\n",
        "    vocGlove[key]=np.array(list(map(float,value)))\n",
        "\n",
        "# normalize the vectors\n",
        "for k,v in vocGlove.items():\n",
        "  v = v/np.linalg.norm(v)\n",
        "  vocGlove[k] = v\n",
        "  vocGloveReverse[str(v)]=k\n"
      ],
      "metadata": {
        "id": "nxXwFJT0rvIK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "dizionario=defaultdict(int)\n",
        "# Replaces escape character with space\n",
        "\n",
        "f = s.split(\"\\n\")\n",
        "data = []\n",
        "# iterate through each sentence in the file\n",
        "frasi=[]\n",
        "for i in f:\n",
        "  copia=i\n",
        "  i = remove_stopwords(i)\n",
        "  i = i.translate(i.maketrans('', '', string.punctuation+\"\\—\\-\\\"\\'\\“\\”\\’\\‘\"))\n",
        "  temp = []\n",
        "\t\n",
        "\t# tokenize the sentence into words\n",
        "  for j in word_tokenize(i):\n",
        "    j = j.lower()\n",
        "    if j in vocGlove.keys() and len(j)>2:\n",
        "      dizionario[j] += 1\n",
        "      temp.append(j)\n",
        "    else: pass\n",
        "  if len(temp)>5:\n",
        "    data.append(temp)\n",
        "    frasi.append(copia)\n",
        "from typing import DefaultDict\n",
        "def generate_dictionary_data(text):\n",
        "    word_to_index= OrderedDict()\n",
        "    index_to_word = OrderedDict()\n",
        "    freq_dict = DefaultDict(int)\n",
        "    corpus = []\n",
        "    corpus_sent = []\n",
        "    count = 0\n",
        "    vocab_size = 0\n",
        "    \n",
        "    for row in text:\n",
        "        for word in row:\n",
        "            freq_dict[word] += 1\n",
        "            word = word.lower()\n",
        "            corpus.append(word)\n",
        "            if word_to_index.get(word) == None:\n",
        "                word_to_index.update ( {word : count})\n",
        "                index_to_word.update ( {count : word })\n",
        "                count  += 1\n",
        "    vocab_size = len(word_to_index)\n",
        "    length_of_corpus = len(corpus)\n",
        "    \n",
        "    return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus,freq_dict"
      ],
      "metadata": {
        "id": "SJvHZ_Nor1uW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index,index_to_word,corpus,vocab_size,length_of_corpus,freq_dict=generate_dictionary_data(data)"
      ],
      "metadata": {
        "id": "t9IISGXZr5sJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentencesAsVec=[]\n",
        "num_sent=len(data)\n",
        "\n",
        "count=0\n",
        "conteggio={}\n",
        "for i in range(num_sent):\n",
        "  len_sent=len(data[i])\n",
        "  vettori=[]\n",
        "  for parola in corpus[count:count+len_sent]:\n",
        "    vettori.append(vocGlove[parola])\n",
        "  conteggio[i]=[count,count+len_sent]\n",
        "  count += len_sent\n",
        "  sentencesAsVec.append(vettori)\n"
      ],
      "metadata": {
        "id": "DtK0Xe5Kr8b6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel distances"
      ],
      "metadata": {
        "id": "IabIFeMMsCXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K=3 #number of Dirac masses in the compressed distribution\n",
        "N=50 #ambient dimension\n",
        "J=1000 #how many samples are included: more the better, otherwise there are oscillations in the SGD/Adam convergence\n",
        "quant=tf.Variable(np.random.rand(N,K),dtype=tf.float32) #this will be the set of quantization variables\n"
      ],
      "metadata": {
        "id": "cz3-lgljsKz7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def empirical_sampling_from_sentence(sentence, S=30):\n",
        "    ''' sample from the sentence (that is a vector of words of length 50) 30 words\n",
        "        note: sentence is a vector of vectors, then the output is a np.array \n",
        "              where each coloumn corresponds to a word\n",
        "    '''\n",
        "    N=len(sentence)\n",
        "    Y=[]\n",
        "    if N>=S:\n",
        "        indices=np.random.choice(N, S)\n",
        "    else:\n",
        "        indices=np.concatenate((np.arange(N),np.random.choice(N,S-N)),axis=0)\n",
        "        #Y=sentence\n",
        "    for i in indices:\n",
        "        Y.append(sentence[i])\n",
        "    Y=np.transpose(np.asarray(Y))\n",
        "    return Y\n",
        "    #return Y,indices\n",
        "\n",
        "empirical_sampling=empirical_sampling_from_sentence\n",
        "\n",
        "def empirical_radon_sobolev_distance_sq(X,Y,local_alphas=None,betas=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2D NxK matrix\n",
        "        input data sample, each column a vector of dimension N, notation X_k\n",
        "    Y : same as X for the second distribution\n",
        "    alphas : 1D array of weights for X, in my case alphas==1/K\n",
        "    betas : same as alphas for Y , in my case betas==1/30?\n",
        "\n",
        "    Note: X,alphas are Tensor/Variable while Y/betas are numpy array\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Radon-Sobolev distance\n",
        "    \n",
        "    '''\n",
        "    N,K=X.shape\n",
        "    Ny,J=Y.shape\n",
        "    Ytensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "\n",
        "    # this is my case.. maybe in the future put tf-idf as weights?\n",
        "    if local_alphas is None:\n",
        "      alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "    else:\n",
        "      alphas=tf.nn.softmax(local_alphas)\n",
        "    if betas is None:\n",
        "        betas = np.ones(J)/J\n",
        "    betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "\n",
        "    # serie di check per vedere che gli input abbiano senso in dimensione\n",
        "    # tf.rank = # indeces required to uniquely select an element\n",
        "    assert (tf.rank(X).numpy()==2) & (Y.ndim==2) & (tf.rank(alphas).numpy()==1) & (betas.ndim==1),\"invalid input dimensions\"\n",
        "    Ka,=alphas.shape\n",
        "    Jb,=betas.shape\n",
        "    assert (N==Ny) & (K==Ka) & (J==Jb), 'invalid input dimensions'\n",
        "\n",
        "    #construct big matrix: [X,Y]=[X_1,...,X_3,Y_1,...,Y_30]\n",
        "    points=tf.concat([X,Ytensor],axis=1)\n",
        "    # construct big vector of weights \n",
        "    gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "\n",
        "    #construct the distances matrix as a K+J x K+J matrix\n",
        "    # h(x)=sqrt(a^2+|x|^2)-a, a=10^-5\n",
        "    distZZ = tf.math.sqrt(1.0e-10+tf.math.reduce_sum(tf.square(tf.expand_dims(points,2)-tf.expand_dims(points,1)),axis=0))-1.0e-5\n",
        "    # Given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed.\n",
        "    # I return thee matrix multiplied by the vectors\n",
        "    return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))\n",
        "\n",
        "def empirical_radon_sobolev_distance_sq_geod(X,Y,local_alphas=None,betas=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2D NxK matrix\n",
        "        input data sample, each column a vector of dimension N, notation X_k\n",
        "    Y : same as X for the second distribution\n",
        "    alphas : 1D array of weights for X, in my case alphas==1/K\n",
        "    betas : same as alphas for Y , in my case betas==1/30?\n",
        "\n",
        "    Note: X,alphas are Tensor/Variable while Y/betas are numpy array\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Radon-Sobolev distance\n",
        "    \n",
        "    '''\n",
        "    N,K=X.shape\n",
        "    Ny,J=Y.shape\n",
        "    Ytensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "    norma=tf.norm(X,axis=0)\n",
        "    X=X/tf.sqrt(norma**2+1.e-6)\n",
        "\n",
        "    if local_alphas is None:\n",
        "      alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "    else:\n",
        "      alphas=tf.nn.softmax(local_alphas)\n",
        "    if betas is None:\n",
        "        betas = np.ones(J)/J\n",
        "    betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "\n",
        "    assert (tf.rank(X).numpy()==2) & (Y.ndim==2) & (tf.rank(alphas).numpy()==1) & (betas.ndim==1),\"invalid input dimensions\"\n",
        "    Ka,=alphas.shape\n",
        "    Jb,=betas.shape\n",
        "    assert (N==Ny)& (K==Ka)&(J==Jb), 'invalid input dimensions'\n",
        "\n",
        "\n",
        "    #construct big matrix: [X,Y]=[X_1,...,X_3,Y_1,...,Y_30]\n",
        "    points=tf.concat([X,Ytensor],axis=1)\n",
        "\n",
        "    # construct big vector of weights \n",
        "    gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "\n",
        "    #construct the distances matrix as a K+J x K+J matrix\n",
        "    \n",
        "    distZZ = tf.math.acos(tf.matmul(tf.transpose(points),points)/1.0001)\n",
        "    # Given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed.\n",
        "    # I return thee matrix multiplied by the vectors\n",
        "    return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))#-sum(np.multiply(Lag,np.power(np.linalg.norm(X,axis=0),2)))+K\n",
        "\n",
        "                            "
      ],
      "metadata": {
        "id": "JI7UI9MTsM2l"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Projection2(vector):\n",
        "    sentence=[vector]\n",
        "    N=len(sentence)\n",
        "    score1={}\n",
        "    for parola in vocGlove.keys():\n",
        "      parola_vec=vocGlove[parola]\n",
        "      compressed=tf.Variable(np.array([parola_vec]).reshape(50,-1), dtype=tf.float32)\n",
        "      score1[parola]=math.sqrt(distanzaH(sentence,compressed).numpy())\n",
        "    best1=min(score1.values())\n",
        "    migliore1=[key for key, value in score1.items() if value == best1]\n",
        "    parola1=vocGlove[migliore1[0]]\n",
        "    return parola1,migliore1[0],best1"
      ],
      "metadata": {
        "id": "VoOBHnJishfZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distanzaH(sentence,compressed):\n",
        "        J=len(sentence)\n",
        "\n",
        "        sentence=np.transpose(sentence)\n",
        "        N,K=compressed.shape\n",
        "        Ytensor = tf.convert_to_tensor(sentence, dtype=tf.float32)\n",
        "        alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "        betas = np.ones(J)/J\n",
        "        betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "        points=tf.concat([compressed,sentence],axis=1)\n",
        "        gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "        distZZ = tf.math.sqrt(1.0e-10+tf.math.reduce_sum(tf.square(tf.expand_dims(points,2)-tf.expand_dims(points,1)),axis=0))-1.0e-5\n",
        "        return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))\n",
        "\n",
        "def distanzaGeo(sentence,compressed):\n",
        "        J=len(sentence)\n",
        "        norma=tf.norm(compressed,axis=0)\n",
        "        compressed=compressed/tf.sqrt(norma**2+1.e-6)\n",
        "        sentence=np.transpose(sentence)\n",
        "        N,K=compressed.shape\n",
        "        Ytensor = tf.convert_to_tensor(sentence, dtype=tf.float32)\n",
        "        alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "        betas = np.ones(J)/J\n",
        "        betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "        points=tf.concat([compressed,sentence],axis=1)\n",
        "        gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "        distZZ = tf.math.acos(tf.matmul(tf.transpose(points),points)/1.0001)\n",
        "        return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))\n",
        "\n"
      ],
      "metadata": {
        "id": "ubqB-8busk7l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Doing some statistics\n",
        "get the distance from compression to the final compression\n",
        "*   S = sentence (mean)\n",
        "*   C1 = compression (raw)\n",
        "*   CP = projected compression\n",
        "*   CR = random compression (mean)\n",
        "\n",
        "I will evaluate d(S,CR), d(S,C1), d(S,CP), d(C1,CP)"
      ],
      "metadata": {
        "id": "f-q1lhA1sqPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Method\n",
        "Given the 3 compression vectors, the associated word is found through the cosine distance. Statistics are reported."
      ],
      "metadata": {
        "id": "t0__BAJGw_Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10,20):\n",
        "    sentence=sentencesAsVec[i]\n",
        "    N=len(sentence)\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.1,beta_1=0.9, beta_2=0.999)# TODO: use lr_schedule for step decay schedule !\n",
        "    Y=empirical_sampling(sentence)\n",
        "    #Y,indices=empirical_sampling(sentence)\n",
        "    loss_quant = lambda: empirical_radon_sobolev_distance_sq(quant,Y)\n",
        "\n",
        "    nr_iter=100\n",
        "    loss_val=[]\n",
        "    opt_val=[]\n",
        "\n",
        "    for sc in range(nr_iter):\n",
        "      step_count = opt.minimize(loss_quant, [quant]).numpy()\n",
        "      opt_val.append(quant.numpy())\n",
        "      loss_val.append(loss_quant())\n",
        "      simili=[]\n",
        "    v1,p1,s1=Projection2(quant[:,0])\n",
        "    v2,p2,s2=Projection2(quant[:,1])\n",
        "    v3,p3,s3=Projection2(quant[:,2])\n",
        "    print(frasi[i])\n",
        "    print(\"[\",p1.upper(),p2.upper(),p3.upper(),\"]\")    \n",
        "    PROJ=tf.Variable(np.array([v1,v2,v3]).reshape(50,-1), dtype=tf.float32)\n",
        "    indici=np.random.choice(N, 3)\n",
        "    RANDOM=tf.Variable(np.array([sentence[indici[0]],sentence[indici[1]],sentence[indici[2]]]).reshape(50,-1),dtype=tf.float32)\n",
        "    CR=tf.Variable(np.random.rand(50,3), dtype=tf.float32)\n",
        "    print(\"RS d(S,C1): \",math.sqrt(distanzaH(sentence,quant).numpy()))\n",
        "    print(\"RS d(S,CP): \",math.sqrt(distanzaH(sentence,PROJ).numpy()))\n",
        "    print(\"RS d(S,CR): \",math.sqrt(distanzaH(sentence,RANDOM).numpy()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmGu2qtzs4l8",
        "outputId": "30f257cf-7efb-4957-8b08-a7541b7aa147"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In retrospect, we should have taken to the streets when McConnell refused to let Obama rightfully fill a Supreme Court vacancy. That was an epic fail on our part as citizens.\n",
            "[ RIGHTFULLY TAKE SUPREME ]\n",
            "RS d(S,C1):  0.29326714895875683\n",
            "RS d(S,CP):  0.5770280586606858\n",
            "RS d(S,CR):  0.5542108879037172\n",
            "Hey, you misguided nincompoops. Here's a glimpse of #FairandBalanced response to a hatemonger https://t.co/inPIz0twO6.\n",
            "[ HATEMONGER HEY MISGUIDED ]\n",
            "RS d(S,C1):  0.32218293949899246\n",
            "RS d(S,CP):  0.5667179959788853\n",
            "RS d(S,CR):  0.5756099482338036\n",
            "Social Security and Medicare are not \"Entitlements\" that can be renegotiated.   The term is \"DEFERRED PAY.\"  I've paid into the system my entire life, and so have you. Anyone stealing your deferred pay needs to be voted out.\n",
            "[ MEDICARE PAY LIFE ]\n",
            "RS d(S,C1):  0.2736783465579387\n",
            "RS d(S,CP):  0.6135981524576426\n",
            "RS d(S,CR):  0.6726400543992391\n",
            "'You let a weird guy whisper in your ear and you didn't trust your daughter.' --my 7-year-old girl https://t.co/Iqhx8PUGSo\n",
            "[ WHISPER EAR DAUGHTER ]\n",
            "RS d(S,C1):  0.3040169403977813\n",
            "RS d(S,CP):  0.6298474442362536\n",
            "RS d(S,CR):  0.6157232128748847\n",
            "@saladinahmed May the initiative to require neutral redistricting in Michigan succeed.   For those unaware of it: https://t.co/DfBydPOnIH.\n",
            "[ MICHIGAN MEASURE SUCCEED ]\n",
            "RS d(S,C1):  0.31035759393630075\n",
            "RS d(S,CP):  0.6658201182402341\n",
            "RS d(S,CR):  0.6247933522967319\n",
            "it's interesting watching this with them because I thought Gollum might scare them but they mostly just get upset when he's mistreated.\n",
            "[ FEEL SCARE GOLLUM ]\n",
            "RS d(S,C1):  0.28351046229791926\n",
            "RS d(S,CP):  0.622398397740996\n",
            "RS d(S,CR):  0.5972712654361384\n",
            "I have but one request this Christmas, guys. Take the #WarOnChristmas hashtag and fill it with accounts of the horrors of battling elves across snowy fields and firing anti-aircraft missiles at sleds streaking overhead.\n",
            "[ FIGHTING SLEDS ELVES ]\n",
            "RS d(S,C1):  0.3465059086624737\n",
            "RS d(S,CP):  0.5603795350074425\n",
            "RS d(S,CR):  0.5946160826143698\n",
            "It is fascinating to see female Democratic senators (with the help of male colleagues) take this coordinated action against Al Franken.\n",
            "[ HELP DEMOCRATS FEMALE ]\n",
            "RS d(S,C1):  0.27727433800140644\n",
            "RS d(S,CP):  0.6409226865651464\n",
            "RS d(S,CR):  0.6353521366392368\n",
            "marvel should make a mockumentary about loki as odin and the behind the scenes drama that went on while loki was trying to produce and direct his play. i wanna see tantrums about sets, actors and interviews with asgardians who are ya we know it's loki\n",
            "[ TURN INTERVIEWS LOKI ]\n",
            "RS d(S,C1):  0.28099356988171265\n",
            "RS d(S,CP):  0.5358211535626362\n",
            "RS d(S,CR):  0.5731270201636001\n",
            "It seems like a whole bunch of narcissistic sexually harassing dudes who have never had to notice all the people around them who cleaned up their messes are suddenly being handed mops.  I'm all for mailing a few mops to the White House, personally.\n",
            "[ SUDDENLY HANDED DUDES ]\n",
            "RS d(S,C1):  0.3016985615950141\n",
            "RS d(S,CP):  0.5750195634665111\n",
            "RS d(S,CR):  0.5847320363521635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best triplet from the sentence\n",
        "Searching for the best triplet of words taken from the sentence, using as kernel \n",
        "$h(x,y)=\\sqrt{|x-y|^2+a^2}-a$."
      ],
      "metadata": {
        "id": "M3Phn4OGtEzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  for w in range(10,20):\n",
        "    sentence=sentencesAsVec[w]\n",
        "    N=len(sentence)\n",
        "    indici=random.sample(range(N), 3)\n",
        "    score={}\n",
        "    for i in range(N):\n",
        "      for j in range(i+1,N):\n",
        "        for k in range(j+1,N):\n",
        "          parole=data[w][i],data[w][j],data[w][k]\n",
        "          compressed=tf.Variable(np.array([sentence[i],sentence[j],sentence[k]]).reshape(50,-1), dtype=tf.float32)\n",
        "          score[parole]=math.sqrt(distanzaH(sentence,compressed).numpy())\n",
        "    best3=sorted(score.values())[:1]\n",
        "    #migliori=[key for key, value in score.items() if value == best3]\n",
        "    print(frasi[w])\n",
        "    migliori=[key for key, value in score.items() if value in best3]\n",
        "    p1=migliori[0][0].upper()\n",
        "    p2=migliori[0][1].upper()\n",
        "    p3=migliori[0][2].upper()\n",
        "    print(\"[\",p1,p2,p3,\"]\",\" d(S,CT)=\",best3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV7C_ao7tZLq",
        "outputId": "0978d9bf-4ff5-4bb0-da62-07d1a84c42c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In retrospect, we should have taken to the streets when McConnell refused to let Obama rightfully fill a Supreme Court vacancy. That was an epic fail on our part as citizens.\n",
            "[ REFUSED COURT EPIC ]  d(S,CT)= [0.5373318575524462]\n",
            "Hey, you misguided nincompoops. Here's a glimpse of #FairandBalanced response to a hatemonger https://t.co/inPIz0twO6.\n",
            "[ HEY NINCOMPOOPS RESPONSE ]  d(S,CT)= [0.5866300178002721]\n",
            "Social Security and Medicare are not \"Entitlements\" that can be renegotiated.   The term is \"DEFERRED PAY.\"  I've paid into the system my entire life, and so have you. Anyone stealing your deferred pay needs to be voted out.\n",
            "[ ENTITLEMENTS PAID ANYONE ]  d(S,CT)= [0.5718857998688053]\n",
            "'You let a weird guy whisper in your ear and you didn't trust your daughter.' --my 7-year-old girl https://t.co/Iqhx8PUGSo\n",
            "[ WEIRD GUY GIRL ]  d(S,CT)= [0.606021407378753]\n",
            "@saladinahmed May the initiative to require neutral redistricting in Michigan succeed.   For those unaware of it: https://t.co/DfBydPOnIH.\n",
            "[ INITIATIVE REQUIRE REDISTRICTING ]  d(S,CT)= [0.610484946362904]\n",
            "it's interesting watching this with them because I thought Gollum might scare them but they mostly just get upset when he's mistreated.\n",
            "[ INTERESTING SCARE UPSET ]  d(S,CT)= [0.5937081372410667]\n",
            "I have but one request this Christmas, guys. Take the #WarOnChristmas hashtag and fill it with accounts of the horrors of battling elves across snowy fields and firing anti-aircraft missiles at sleds streaking overhead.\n",
            "[ ANTIAIRCRAFT MISSILES SLEDS ]  d(S,CT)= [0.49686926952392435]\n",
            "It is fascinating to see female Democratic senators (with the help of male colleagues) take this coordinated action against Al Franken.\n",
            "[ FASCINATING COLLEAGUES FRANKEN ]  d(S,CT)= [0.5865067323686917]\n",
            "marvel should make a mockumentary about loki as odin and the behind the scenes drama that went on while loki was trying to produce and direct his play. i wanna see tantrums about sets, actors and interviews with asgardians who are ya we know it's loki\n",
            "[ TANTRUMS ACTORS LOKI ]  d(S,CT)= [0.5163904923474608]\n",
            "It seems like a whole bunch of narcissistic sexually harassing dudes who have never had to notice all the people around them who cleaned up their messes are suddenly being handed mops.  I'm all for mailing a few mops to the White House, personally.\n",
            "[ CLEANED MESSES SUDDENLY ]  d(S,CT)= [0.5336454743411349]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using as kernel \n",
        "$h(x,y)=\\arccos{x\\cdot y}$"
      ],
      "metadata": {
        "id": "PsBPAOTnthxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  for w in range(10,20):\n",
        "    sentence=sentencesAsVec[w]\n",
        "    N=len(sentence)\n",
        "    indici=random.sample(range(N), 3)\n",
        "    score={}\n",
        "    for i in range(N):\n",
        "      for j in range(i+1,N):\n",
        "        for k in range(j+1,N):\n",
        "          parole=data[w][i],data[w][j],data[w][k]\n",
        "          compressed=tf.Variable(np.array([sentence[i],sentence[j],sentence[k]]).reshape(50,-1), dtype=tf.float32)\n",
        "          score[parole]=math.sqrt(distanzaGeo(sentence,compressed).numpy())\n",
        "    best3=sorted(score.values())[:1]\n",
        "    print(frasi[w])\n",
        "    migliori=[key for key, value in score.items() if value in best3]\n",
        "    p1=migliori[0][0].upper()\n",
        "    p2=migliori[0][1].upper()\n",
        "    p3=migliori[0][2].upper()\n",
        "    print(\"[\",p1,p2,p3,\"]\",\" d(S,CT)=\",best3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3D0cTEzuANT",
        "outputId": "80251d9f-70c2-425f-837e-46e7c8d3cc76"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In retrospect, we should have taken to the streets when McConnell refused to let Obama rightfully fill a Supreme Court vacancy. That was an epic fail on our part as citizens.\n",
            "[ REFUSED COURT EPIC ]  d(S,CT)= [0.5544603715551787]\n",
            "Hey, you misguided nincompoops. Here's a glimpse of #FairandBalanced response to a hatemonger https://t.co/inPIz0twO6.\n",
            "[ HEY NINCOMPOOPS RESPONSE ]  d(S,CT)= [0.6147604676278882]\n",
            "Social Security and Medicare are not \"Entitlements\" that can be renegotiated.   The term is \"DEFERRED PAY.\"  I've paid into the system my entire life, and so have you. Anyone stealing your deferred pay needs to be voted out.\n",
            "[ ENTITLEMENTS PAID ANYONE ]  d(S,CT)= [0.5906362451764492]\n",
            "'You let a weird guy whisper in your ear and you didn't trust your daughter.' --my 7-year-old girl https://t.co/Iqhx8PUGSo\n",
            "[ WEIRD GUY GIRL ]  d(S,CT)= [0.6341758922936803]\n",
            "@saladinahmed May the initiative to require neutral redistricting in Michigan succeed.   For those unaware of it: https://t.co/DfBydPOnIH.\n",
            "[ MAY REQUIRE MICHIGAN ]  d(S,CT)= [0.6457552298623149]\n",
            "it's interesting watching this with them because I thought Gollum might scare them but they mostly just get upset when he's mistreated.\n",
            "[ INTERESTING SCARE UPSET ]  d(S,CT)= [0.6199618407781209]\n",
            "I have but one request this Christmas, guys. Take the #WarOnChristmas hashtag and fill it with accounts of the horrors of battling elves across snowy fields and firing anti-aircraft missiles at sleds streaking overhead.\n",
            "[ ANTIAIRCRAFT MISSILES SLEDS ]  d(S,CT)= [0.5035085957275477]\n",
            "It is fascinating to see female Democratic senators (with the help of male colleagues) take this coordinated action against Al Franken.\n",
            "[ FASCINATING COLLEAGUES FRANKEN ]  d(S,CT)= [0.6120226878389486]\n",
            "marvel should make a mockumentary about loki as odin and the behind the scenes drama that went on while loki was trying to produce and direct his play. i wanna see tantrums about sets, actors and interviews with asgardians who are ya we know it's loki\n",
            "[ TANTRUMS ACTORS LOKI ]  d(S,CT)= [0.5314089032549687]\n",
            "It seems like a whole bunch of narcissistic sexually harassing dudes who have never had to notice all the people around them who cleaned up their messes are suddenly being handed mops.  I'm all for mailing a few mops to the White House, personally.\n",
            "[ NOTICE PEOPLE SUDDENLY ]  d(S,CT)= [0.5562825975617672]\n"
          ]
        }
      ]
    }
  ]
}