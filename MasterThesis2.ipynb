{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MasterThesis2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMpJI6+WfDmdAyM1k9MVF6v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ceciliasecchi/MasterThesis/blob/main/MasterThesis2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osN7yftnjqAY",
        "outputId": "bd739390-ace5-4aac-99e7-eec481853b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wordfreq in /usr/local/lib/python3.7/dist-packages (3.0.1)\n",
            "Requirement already satisfied: regex>=2020.04.04 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (2022.6.2)\n",
            "Requirement already satisfied: ftfy>=6.1 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (6.1.1)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (1.0.4)\n",
            "Requirement already satisfied: langcodes>=3.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq) (3.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy>=6.1->wordfreq) (0.2.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install wordfreq\n",
        "from wordfreq import word_frequency\n",
        "from nltk.collections import OrderedDict\n",
        "import gensim\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import re  # For preprocessing\n",
        "import string\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import csv\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "from gensim.parsing.preprocessing import remove_stopwords,preprocess_string,preprocess_documents\n",
        "warnings.filterwarnings(action = 'ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import collections\n",
        "import math\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import bernoulli \n",
        "from scipy.special import softmax as scipy_softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopp = {'and','but','this','said','you','the','how','for','like','thats','hes',\n",
        "         'like','thought','again','but','dont','that','ive','didnt','they','she',\n",
        "         'while','instead','would','thats','anything','its','hasnt','still',\n",
        "         'gets','went',\"wouldnt\",\"get\",\"let\",'way',\"these\",\"those\",\"havent\"\n",
        "         'looked','came','not','got','then','ive',\"were\",'there','wont'}\n",
        "\n",
        "# Reads 'tweets.txt’ file\n",
        "sample = open(\"tweets.txt\", \"r\")\n",
        "s = sample.read()\n",
        "\n",
        "voc = {}\n",
        "vocGlove = OrderedDict()\n",
        "vocGloveReverse = {}\n",
        "with open('vocGlove_tweets.csv', mode='r') as inp:\n",
        "    reader = csv.reader(inp)\n",
        "    for row in reader:\n",
        "      voc = {rows[0]:rows[1].replace(\"\\n\", \" \").replace(\"[\",\" \").replace(\"]\",\" \") for rows in reader}\n",
        "    \n",
        "for key,value in voc.items():\n",
        "  value=value.split()\n",
        "  if len(value)!=50:\n",
        "    print(key)\n",
        "  if key not in stopp:\n",
        "    vocGlove[key]=np.array(list(map(float,value)))\n",
        "\n",
        "# normalize the vectors\n",
        "for k,v in vocGlove.items():\n",
        "  v = v/np.linalg.norm(v)\n",
        "  vocGlove[k] = v\n",
        "  vocGloveReverse[str(v)]=k\n"
      ],
      "metadata": {
        "id": "nxXwFJT0rvIK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "dizionario=defaultdict(int)\n",
        "# Replaces escape character with space\n",
        "\n",
        "f = s.split(\"\\n\")\n",
        "data = []\n",
        "# iterate through each sentence in the file\n",
        "frasi=[]\n",
        "for i in f:\n",
        "  copia=i\n",
        "  i = remove_stopwords(i)\n",
        "  i = i.translate(i.maketrans('', '', string.punctuation+\"\\—\\-\\\"\\'\\“\\”\\’\\‘\"))\n",
        "  temp = []\n",
        "\t\n",
        "\t# tokenize the sentence into words\n",
        "  for j in word_tokenize(i):\n",
        "    j = j.lower()\n",
        "    if j in vocGlove.keys() and len(j)>2:\n",
        "      dizionario[j] += 1\n",
        "      temp.append(j)\n",
        "    else: pass\n",
        "  if len(temp)>5:\n",
        "    data.append(temp)\n",
        "    frasi.append(copia)\n",
        "from typing import DefaultDict\n",
        "def generate_dictionary_data(text):\n",
        "    word_to_index= OrderedDict()\n",
        "    index_to_word = OrderedDict()\n",
        "    freq_dict = DefaultDict(int)\n",
        "    corpus = []\n",
        "    corpus_sent = []\n",
        "    count = 0\n",
        "    vocab_size = 0\n",
        "    \n",
        "    for row in text:\n",
        "        for word in row:\n",
        "            freq_dict[word] += 1\n",
        "            word = word.lower()\n",
        "            corpus.append(word)\n",
        "            if word_to_index.get(word) == None:\n",
        "                word_to_index.update ( {word : count})\n",
        "                index_to_word.update ( {count : word })\n",
        "                count  += 1\n",
        "    vocab_size = len(word_to_index)\n",
        "    length_of_corpus = len(corpus)\n",
        "    \n",
        "    return word_to_index,index_to_word,corpus,vocab_size,length_of_corpus,freq_dict"
      ],
      "metadata": {
        "id": "SJvHZ_Nor1uW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_index,index_to_word,corpus,vocab_size,length_of_corpus,freq_dict=generate_dictionary_data(data)"
      ],
      "metadata": {
        "id": "t9IISGXZr5sJ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentencesAsVec=[]\n",
        "num_sent=len(data)\n",
        "\n",
        "count=0\n",
        "conteggio={}\n",
        "for i in range(num_sent):\n",
        "  len_sent=len(data[i])\n",
        "  vettori=[]\n",
        "  for parola in corpus[count:count+len_sent]:\n",
        "    vettori.append(vocGlove[parola])\n",
        "  conteggio[i]=[count,count+len_sent]\n",
        "  count += len_sent\n",
        "  sentencesAsVec.append(vettori)\n"
      ],
      "metadata": {
        "id": "DtK0Xe5Kr8b6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kernel distances"
      ],
      "metadata": {
        "id": "IabIFeMMsCXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K=3 #number of Dirac masses in the compressed distribution\n",
        "N=50 #ambient dimension\n",
        "J=1000 #how many samples are included: more the better, otherwise there are oscillations in the SGD/Adam convergence\n",
        "quant=tf.Variable(np.random.rand(N,K),dtype=tf.float32) #this will be the set of quantization variables\n"
      ],
      "metadata": {
        "id": "cz3-lgljsKz7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def empirical_sampling_from_sentence(sentence, S=30):\n",
        "    ''' sample from the sentence (that is a vector of words of length 50) 30 words\n",
        "        note: sentence is a vector of vectors, then the output is a np.array \n",
        "              where each coloumn corresponds to a word\n",
        "    '''\n",
        "    N=len(sentence)\n",
        "    Y=[]\n",
        "    if N>=S:\n",
        "        indices=np.random.choice(N, S)\n",
        "    else:\n",
        "        indices=np.concatenate((np.arange(N),np.random.choice(N,S-N)),axis=0)\n",
        "        #Y=sentence\n",
        "    for i in indices:\n",
        "        Y.append(sentence[i])\n",
        "    Y=np.transpose(np.asarray(Y))\n",
        "    return Y\n",
        "    #return Y,indices\n",
        "\n",
        "empirical_sampling=empirical_sampling_from_sentence\n",
        "\n",
        "def empirical_radon_sobolev_distance_sq(X,Y,local_alphas=None,betas=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2D NxK matrix\n",
        "        input data sample, each column a vector of dimension N, notation X_k\n",
        "    Y : same as X for the second distribution\n",
        "    alphas : 1D array of weights for X, in my case alphas==1/K\n",
        "    betas : same as alphas for Y , in my case betas==1/30?\n",
        "\n",
        "    Note: X,alphas are Tensor/Variable while Y/betas are numpy array\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Radon-Sobolev distance\n",
        "    \n",
        "    '''\n",
        "    N,K=X.shape\n",
        "    Ny,J=Y.shape\n",
        "    Ytensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "\n",
        "    # this is my case.. maybe in the future put tf-idf as weights?\n",
        "    if local_alphas is None:\n",
        "      alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "    else:\n",
        "      alphas=tf.nn.softmax(local_alphas)\n",
        "    if betas is None:\n",
        "        betas = np.ones(J)/J\n",
        "    betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "\n",
        "    # serie di check per vedere che gli input abbiano senso in dimensione\n",
        "    # tf.rank = # indeces required to uniquely select an element\n",
        "    assert (tf.rank(X).numpy()==2) & (Y.ndim==2) & (tf.rank(alphas).numpy()==1) & (betas.ndim==1),\"invalid input dimensions\"\n",
        "    Ka,=alphas.shape\n",
        "    Jb,=betas.shape\n",
        "    assert (N==Ny) & (K==Ka) & (J==Jb), 'invalid input dimensions'\n",
        "\n",
        "    #construct big matrix: [X,Y]=[X_1,...,X_3,Y_1,...,Y_30]\n",
        "    points=tf.concat([X,Ytensor],axis=1)\n",
        "    # construct big vector of weights \n",
        "    gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "\n",
        "    #construct the distances matrix as a K+J x K+J matrix\n",
        "    # h(x)=sqrt(a^2+|x|^2)-a, a=10^-5\n",
        "    distZZ = tf.math.sqrt(1.0e-10+tf.math.reduce_sum(tf.square(tf.expand_dims(points,2)-tf.expand_dims(points,1)),axis=0))-1.0e-5\n",
        "    # Given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed.\n",
        "    # I return thee matrix multiplied by the vectors\n",
        "    return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))\n",
        "\n",
        "def empirical_radon_sobolev_distance_sq_geod(X,Y,local_alphas=None,betas=None):\n",
        "    '''\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 2D NxK matrix\n",
        "        input data sample, each column a vector of dimension N, notation X_k\n",
        "    Y : same as X for the second distribution\n",
        "    alphas : 1D array of weights for X, in my case alphas==1/K\n",
        "    betas : same as alphas for Y , in my case betas==1/30?\n",
        "\n",
        "    Note: X,alphas are Tensor/Variable while Y/betas are numpy array\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Radon-Sobolev distance\n",
        "    \n",
        "    '''\n",
        "    N,K=X.shape\n",
        "    Ny,J=Y.shape\n",
        "    Ytensor = tf.convert_to_tensor(Y, dtype=tf.float32)\n",
        "    norma=tf.norm(X,axis=0)\n",
        "    X=X/tf.sqrt(norma**2+1.e-6)\n",
        "\n",
        "    if local_alphas is None:\n",
        "      alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "    else:\n",
        "      alphas=tf.nn.softmax(local_alphas)\n",
        "    if betas is None:\n",
        "        betas = np.ones(J)/J\n",
        "    betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "\n",
        "    assert (tf.rank(X).numpy()==2) & (Y.ndim==2) & (tf.rank(alphas).numpy()==1) & (betas.ndim==1),\"invalid input dimensions\"\n",
        "    Ka,=alphas.shape\n",
        "    Jb,=betas.shape\n",
        "    assert (N==Ny)& (K==Ka)&(J==Jb), 'invalid input dimensions'\n",
        "\n",
        "\n",
        "    #construct big matrix: [X,Y]=[X_1,...,X_3,Y_1,...,Y_30]\n",
        "    points=tf.concat([X,Ytensor],axis=1)\n",
        "\n",
        "    # construct big vector of weights \n",
        "    gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "\n",
        "    #construct the distances matrix as a K+J x K+J matrix\n",
        "    \n",
        "    distZZ = tf.math.acos(tf.matmul(tf.transpose(points),points)/1.0001)\n",
        "    # Given a tensor input, this operation returns a tensor of the same type with all dimensions of size 1 removed.\n",
        "    # I return thee matrix multiplied by the vectors\n",
        "    return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))#-sum(np.multiply(Lag,np.power(np.linalg.norm(X,axis=0),2)))+K\n",
        "\n",
        "                            "
      ],
      "metadata": {
        "id": "JI7UI9MTsM2l"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Projection(vector):\n",
        "  vicini=[]\n",
        "  lista=[]\n",
        "  chiavi=list(word_to_index.keys())\n",
        "\n",
        "  for word in chiavi:\n",
        "    pr = np.dot(vector, vocGlove[word])\n",
        "    lista.append(pr)\n",
        "  maxs=[]\n",
        "  for i in range(num):\n",
        "    mm=max(lista)\n",
        "    mm_index=lista.index(mm)\n",
        "    maxs.append(mm_index)\n",
        "    lista.remove(mm)\n",
        "  for i in maxs:\n",
        "    vicini.append(chiavi[i])\n",
        "  return vicini  \n",
        "\n",
        "def Projection2(vector):\n",
        "    #vector=np.array([vector]).reshape(-1,50)\n",
        "    #vector=[vector]\n",
        "    sentence=[vector]\n",
        "    N=len(sentence)\n",
        "    #indici=random.sample(range(N), 3)\n",
        "    score1={}\n",
        "    for parola in vocGlove.keys():\n",
        "      parola_vec=vocGlove[parola]\n",
        "      compressed=tf.Variable(np.array([parola_vec]).reshape(50,-1), dtype=tf.float32)\n",
        "      score1[parola]=math.sqrt(distanzaH(sentence,compressed).numpy())\n",
        "    best1=min(score1.values())\n",
        "    migliore1=[key for key, value in score1.items() if value == best1]\n",
        "    parola1=vocGlove[migliore1[0]]\n",
        "    return parola1,migliore1[0],best1"
      ],
      "metadata": {
        "id": "VoOBHnJishfZ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def distanzaH(sentence,compressed):\n",
        "        J=len(sentence)\n",
        "\n",
        "        sentence=np.transpose(sentence)\n",
        "        N,K=compressed.shape\n",
        "        Ytensor = tf.convert_to_tensor(sentence, dtype=tf.float32)\n",
        "        alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "        betas = np.ones(J)/J\n",
        "        betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "        points=tf.concat([compressed,sentence],axis=1)\n",
        "        gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "        distZZ = tf.math.sqrt(1.0e-10+tf.math.reduce_sum(tf.square(tf.expand_dims(points,2)-tf.expand_dims(points,1)),axis=0))-1.0e-5\n",
        "        return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))\n",
        "\n",
        "def distanzaGeo(sentence,compressed):\n",
        "        J=len(sentence)\n",
        "        norma=tf.norm(compressed,axis=0)\n",
        "        compressed=compressed/tf.sqrt(norma**2+1.e-6)\n",
        "        sentence=np.transpose(sentence)\n",
        "        N,K=compressed.shape\n",
        "        Ytensor = tf.convert_to_tensor(sentence, dtype=tf.float32)\n",
        "        alphas = tf.convert_to_tensor(np.ones(K)/K,dtype=tf.float32)\n",
        "        betas = np.ones(J)/J\n",
        "        betastensor = tf.convert_to_tensor(betas, dtype=tf.float32)\n",
        "        points=tf.concat([compressed,sentence],axis=1)\n",
        "        gammas=tf.concat([alphas,-betastensor],axis=0)\n",
        "        distZZ = tf.math.acos(tf.matmul(tf.transpose(points),points)/1.0001)\n",
        "        return tf.squeeze(-0.5*tf.expand_dims(gammas,0)@distZZ@tf.expand_dims(gammas,1))\n",
        "\n"
      ],
      "metadata": {
        "id": "ubqB-8busk7l"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Doing some statistics\n",
        "get the distance from compression to the final compression\n",
        "*   S = sentence (mean)\n",
        "*   C1 = compression (raw)\n",
        "*   CP = projected compression\n",
        "*   CR = random compression (mean)\n",
        "\n",
        "I will evaluate d(S,CR), d(S,C1), d(S,CP), d(C1,CP)"
      ],
      "metadata": {
        "id": "f-q1lhA1sqPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Method\n",
        "Given the 3 compression vectors, the associated word is found through the cosine distance. Statistics are reported."
      ],
      "metadata": {
        "id": "t0__BAJGw_Mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20,40):\n",
        "    sentence=sentencesAsVec[i]\n",
        "    N=len(sentence)\n",
        "\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=0.1,beta_1=0.9, beta_2=0.999)# TODO: use lr_schedule for step decay schedule !\n",
        "    Y=empirical_sampling(sentence)\n",
        "    #Y,indices=empirical_sampling(sentence)\n",
        "    loss_quant = lambda: empirical_radon_sobolev_distance_sq(quant,Y)\n",
        "\n",
        "    nr_iter=100\n",
        "    loss_val=[]\n",
        "    opt_val=[]\n",
        "\n",
        "    for sc in range(nr_iter):\n",
        "      step_count = opt.minimize(loss_quant, [quant]).numpy()\n",
        "      opt_val.append(quant.numpy())\n",
        "      loss_val.append(loss_quant())\n",
        "      simili=[]\n",
        "    v1,p1,s1=Projection2(quant[:,0])\n",
        "    v2,p2,s2=Projection2(quant[:,1])\n",
        "    v3,p3,s3=Projection2(quant[:,2])\n",
        "    print(frasi[i])\n",
        "    print(\"[\",p1.upper(),p2.upper(),p3.upper(),\"]\")    \n",
        "    PROJ=tf.Variable(np.array([v1,v2,v3]).reshape(50,-1), dtype=tf.float32)\n",
        "    indici=np.random.choice(N, 3)\n",
        "    RANDOM=tf.Variable(np.array([sentence[indici[0]],sentence[indici[1]],sentence[indici[2]]]).reshape(50,-1),dtype=tf.float32)\n",
        "    CR=tf.Variable(np.random.rand(50,3), dtype=tf.float32)\n",
        "    print(\"RS d(S,C1): \",math.sqrt(distanzaH(sentence,quant).numpy()))\n",
        "    print(\"RS d(S,CP): \",math.sqrt(distanzaH(sentence,PROJ).numpy()))\n",
        "    print(\"RS d(S,CR): \",math.sqrt(distanzaH(sentence,RANDOM).numpy()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmGu2qtzs4l8",
        "outputId": "16675041-94ec-4786-a0b1-0f27bbcc3931"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry you don't like being asked to clean up after yourselves, dudes.   Accountability is hard.  Oh, well.\n",
            "[ YOURSELVES HARD CLEAN ]\n",
            "RS d(S,C1):  0.28058991856481147\n",
            "RS d(S,CP):  0.6346816961116057\n",
            "RS d(S,CR):  0.653727884702632\n",
            "A major cause for celebration and dancing in the streets: Henry the Hatter to open in Eastern Market on Friday https://t.co/lOaHpbv7xP @crainsdetroit\n",
            "[ DANCING EASTERN FRIDAY ]\n",
            "RS d(S,C1):  0.3030915522064531\n",
            "RS d(S,CP):  0.633876259658275\n",
            "RS d(S,CR):  0.6525984169924163\n",
            "my seven-year-old artist daughter has been drawing a lot of cartoon bunnies lately. they've all been extremely cutesey, but today I found this... https://t.co/1jsraU01Nc\n",
            "[ ARTIST EXTREMELY LOT ]\n",
            "RS d(S,C1):  0.3019409611510891\n",
            "RS d(S,CP):  0.6955418369248291\n",
            "RS d(S,CR):  0.6165946334145038\n",
            "A couple years ago, a very nice young girl was being bullied for wearing a feminist t-shirt in a photo. I offered her words of support.   Today, in Sao Paulo, she proudly handed me a graphic novel she wrote.  I love comics.\n",
            "[ WROTE SAO LOVE ]\n",
            "RS d(S,C1):  0.29383012662904123\n",
            "RS d(S,CP):  0.6105204112150118\n",
            "RS d(S,CR):  0.6114956318550332\n",
            "I first saw it as Danny DeVito looking outside the window, disappointed to see a topless Woman wearing the same speedo he did. https://t.co/NjpvURZtae\n",
            "[ LOOKING DANNY TOPLESS ]\n",
            "RS d(S,C1):  0.2833435368219981\n",
            "RS d(S,CP):  0.6213274582022638\n",
            "RS d(S,CR):  0.5867862403544873\n",
            "managed to get out and take the kids sledding in the two hour or so window today where there was enough snow to do that feasibly  which was wonderful but now I'm sore as shit\n",
            "[ WINDOW SHIT SORE ]\n",
            "RS d(S,C1):  0.31831966924842275\n",
            "RS d(S,CP):  0.5819338518613604\n",
            "RS d(S,CR):  0.6153943050749567\n",
            "Stay strong, Keaton. Don't let them make you turn cold. I promise it gets better. While those punks at your school are deciding what kind of people they want to be in this world, how would you and your mom like to come to the Avengers premiere in LA next year? https://t.co/s1QwCQ3toi\n",
            "[ COME KEATON WORLD ]\n",
            "RS d(S,C1):  0.27735045250329116\n",
            "RS d(S,CP):  0.6414472141729007\n",
            "RS d(S,CR):  0.6460613007336447\n",
            "20.94% Corp. rate to pay for tax cut for working family making $40k was anti-growth but 21% to cut tax for couples making $1million is fine?\n",
            "[ TAX 2094 UP ]\n",
            "RS d(S,C1):  0.3091200192023601\n",
            "RS d(S,CP):  0.6184613084185896\n",
            "RS d(S,CR):  0.590213028466854\n",
            "I think sports is ready for multi-league trades. Detroit could trade Ian Kinsler and Ish Smith to Los Angeles for a running back and a defenseman.\n",
            "[ NEW KINSLER RUNNING ]\n",
            "RS d(S,C1):  0.3059164839199944\n",
            "RS d(S,CP):  0.6278219649729769\n",
            "RS d(S,CR):  0.5876877647009033\n",
            "[Punches Matt Damon in the face] At least I didn't kick you in the balls.\n",
            "[ KICK MATT BALLS ]\n",
            "RS d(S,C1):  0.2778145729305138\n",
            "RS d(S,CP):  0.6822133759480933\n",
            "RS d(S,CR):  0.6615709349140036\n",
            "I'm sure sexual assault charges in the military would decrease if men didn't serve https://t.co/JL2AHLB5B2\n",
            "[ ASSAULT SEXUAL SERVE ]\n",
            "RS d(S,C1):  0.2686401231288798\n",
            "RS d(S,CP):  0.6961662537291421\n",
            "RS d(S,CR):  0.6799563391426946\n",
            "Dear Matt Damon,  It's the micro that makes the macro.  (Thread)\n",
            "[ MATT THREAD MAKES ]\n",
            "RS d(S,C1):  0.2945800973470774\n",
            "RS d(S,CP):  0.6150564269409601\n",
            "RS d(S,CR):  0.585370666986395\n",
            "We need to fix this country's infrastructure. It should be a major priority for the president and Congress. https://t.co/l3xSM8ZSWi\n",
            "[ PRESIDENT PRIORITY FIX ]\n",
            "RS d(S,C1):  0.2812379728500723\n",
            "RS d(S,CP):  0.6799086726091541\n",
            "RS d(S,CR):  0.6915753044739191\n",
            "The more I see of Disney's 'Hall of Presidents' Trump, the more I'm convinced they made a Hillary one first and had to redo it https://t.co/vJTMnZPak8\n",
            "[ HILLARY TRUMP DISNEYS ]\n",
            "RS d(S,C1):  0.29960440491056606\n",
            "RS d(S,CP):  0.6071485290743251\n",
            "RS d(S,CR):  0.6475134090375448\n",
            "Ok if you are going to follow this account we have some rules  1[?] You will need a visitor sticker please pick one up in the lobby 2[?] No hitting not even pretend hitting 3[?] Do not get too loud or I will blow the moon whistle  Hit like once you have read all the rules\n",
            "[ HIT DO LOBBY ]\n",
            "RS d(S,C1):  0.28719092527885154\n",
            "RS d(S,CP):  0.6256780284469158\n",
            "RS d(S,CR):  0.688992289558143\n",
            "@mzbat My wife makes more money than me.  Claims it means she gets to set the thermostat in the house.  \n",
            "[ SET MAKES HOUSE ]\n",
            "RS d(S,C1):  0.27589162611543433\n",
            "RS d(S,CP):  0.6485278974281085\n",
            "RS d(S,CR):  0.6002460541458431\n",
            "Dear Brave Airport Lady, if you ever see this, just know that although I'm sorry you went through this (esp during the holidays), I hope that, one day, you look back on this as a defining moment in your life. Never settle. Follow your heart. You are a goddess. [?]\n",
            "[ AIRPORT LOOK LOVE ]\n",
            "RS d(S,C1):  0.2941699389558469\n",
            "RS d(S,CP):  0.625591975244594\n",
            "RS d(S,CR):  0.6509788533744709\n",
            "New rule: for every article about a white guy who hates the new Star Wars because he thinks competent women are 'unrealistic' and he can't identify with heroes of color, you have to write an article about the joy and wonder of fans seeing themselves represented.\n",
            "[ NEW WONDER ARTICLE ]\n",
            "RS d(S,C1):  0.2794458085558856\n",
            "RS d(S,CP):  0.6030060675570394\n",
            "RS d(S,CR):  0.6197517098976051\n",
            "My mom texted me from Walmart to see if i wanted beer for Christmas weekend and included some choices. One was Founders Dirty Bastard. My mom texted the word \"Bastard\" and I'm dead.\n",
            "[ HOME TEXTED WORD ]\n",
            "RS d(S,C1):  0.2972094007699561\n",
            "RS d(S,CP):  0.5847448035682439\n",
            "RS d(S,CR):  0.5949525701227353\n",
            "When a movie is blasted by a minority group, I listen and analyze. How can we do better?   When a movie is blasted by angry men I buy 2 tickets\n",
            "[ BUY POLITICIANS WHEN ]\n",
            "RS d(S,C1):  0.290212649922378\n",
            "RS d(S,CP):  0.6135876612971947\n",
            "RS d(S,CR):  0.6334142061306262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best triplet from the sentence\n",
        "Searching for the best triplet of words taken from the sentence, using as kernel \n",
        "$h(x,y)=\\sqrt{|x-y|^2+a^2}-a$."
      ],
      "metadata": {
        "id": "M3Phn4OGtEzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  for w in range(10,20):\n",
        "    sentence=sentencesAsVec[w]\n",
        "    N=len(sentence)\n",
        "    indici=random.sample(range(N), 3)\n",
        "    score={}\n",
        "    for i in range(N):\n",
        "      for j in range(i+1,N):\n",
        "        for k in range(j+1,N):\n",
        "          parole=data[w][i],data[w][j],data[w][k]\n",
        "          compressed=tf.Variable(np.array([sentence[i],sentence[j],sentence[k]]).reshape(50,-1), dtype=tf.float32)\n",
        "          score[parole]=math.sqrt(distanzaH(sentence,compressed).numpy())\n",
        "    best3=sorted(score.values())[:1]\n",
        "    #migliori=[key for key, value in score.items() if value == best3]\n",
        "    print(frasi[w])\n",
        "    migliori=[key for key, value in score.items() if value in best3]\n",
        "    p1=migliori[0][0].upper()\n",
        "    p2=migliori[0][1].upper()\n",
        "    p3=migliori[0][2].upper()\n",
        "    print(\"[\",p1,p2,p3,\"]\",\" d(S,CT)=\",best3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV7C_ao7tZLq",
        "outputId": "2883c3cb-66b9-4937-caa0-8a8c939f0d9d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In retrospect, we should have taken to the streets when McConnell refused to let Obama rightfully fill a Supreme Court vacancy. That was an epic fail on our part as citizens.\n",
            "[ REFUSED COURT EPIC ]  d(S,CT)= [0.5373318852842112]\n",
            "Hey, you misguided nincompoops. Here's a glimpse of #FairandBalanced response to a hatemonger https://t.co/inPIz0twO6.\n",
            "[ HEY NINCOMPOOPS RESPONSE ]  d(S,CT)= [0.5866300686028567]\n",
            "Social Security and Medicare are not \"Entitlements\" that can be renegotiated.   The term is \"DEFERRED PAY.\"  I've paid into the system my entire life, and so have you. Anyone stealing your deferred pay needs to be voted out.\n",
            "[ ENTITLEMENTS PAID ANYONE ]  d(S,CT)= [0.5718858259249879]\n",
            "'You let a weird guy whisper in your ear and you didn't trust your daughter.' --my 7-year-old girl https://t.co/Iqhx8PUGSo\n",
            "[ WEIRD GUY GIRL ]  d(S,CT)= [0.606021407378753]\n",
            "@saladinahmed May the initiative to require neutral redistricting in Michigan succeed.   For those unaware of it: https://t.co/DfBydPOnIH.\n",
            "[ INITIATIVE REQUIRE REDISTRICTING ]  d(S,CT)= [0.6104849951803587]\n",
            "it's interesting watching this with them because I thought Gollum might scare them but they mostly just get upset when he's mistreated.\n",
            "[ INTERESTING SCARE UPSET ]  d(S,CT)= [0.5937081121426041]\n",
            "I have but one request this Christmas, guys. Take the #WarOnChristmas hashtag and fill it with accounts of the horrors of battling elves across snowy fields and firing anti-aircraft missiles at sleds streaking overhead.\n",
            "[ ANTIAIRCRAFT MISSILES SLEDS ]  d(S,CT)= [0.4968692995140277]\n",
            "It is fascinating to see female Democratic senators (with the help of male colleagues) take this coordinated action against Al Franken.\n",
            "[ FASCINATING COLLEAGUES FRANKEN ]  d(S,CT)= [0.5865067577753239]\n",
            "marvel should make a mockumentary about loki as odin and the behind the scenes drama that went on while loki was trying to produce and direct his play. i wanna see tantrums about sets, actors and interviews with asgardians who are ya we know it's loki\n",
            "[ TANTRUMS ACTORS LOKI ]  d(S,CT)= [0.5163904923474608]\n",
            "It seems like a whole bunch of narcissistic sexually harassing dudes who have never had to notice all the people around them who cleaned up their messes are suddenly being handed mops.  I'm all for mailing a few mops to the White House, personally.\n",
            "[ CLEANED MESSES SUDDENLY ]  d(S,CT)= [0.5336454743411349]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using as kernel \n",
        "$h(x,y)=\\arccos{x\\cdot y}$"
      ],
      "metadata": {
        "id": "PsBPAOTnthxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  for w in range(10,20):\n",
        "    sentence=sentencesAsVec[w]\n",
        "    N=len(sentence)\n",
        "    indici=random.sample(range(N), 3)\n",
        "    score={}\n",
        "    for i in range(N):\n",
        "      for j in range(i+1,N):\n",
        "        for k in range(j+1,N):\n",
        "          parole=data[w][i],data[w][j],data[w][k]\n",
        "          compressed=tf.Variable(np.array([sentence[i],sentence[j],sentence[k]]).reshape(50,-1), dtype=tf.float32)\n",
        "          score[parole]=math.sqrt(distanzaGeo(sentence,compressed).numpy())\n",
        "    best3=sorted(score.values())[:1]\n",
        "    print(frasi[w])\n",
        "    migliori=[key for key, value in score.items() if value in best3]\n",
        "    p1=migliori[0][0].upper()\n",
        "    p2=migliori[0][1].upper()\n",
        "    p3=migliori[0][2].upper()\n",
        "    print(\"[\",p1,p2,p3,\"]\",\" d(S,CT)=\",best3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3D0cTEzuANT",
        "outputId": "cebb55da-2508-4d27-dd22-a43e3b19fdc6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In retrospect, we should have taken to the streets when McConnell refused to let Obama rightfully fill a Supreme Court vacancy. That was an epic fail on our part as citizens.\n",
            "[ REFUSED COURT EPIC ]  d(S,CT)= [0.5544603715551787]\n",
            "Hey, you misguided nincompoops. Here's a glimpse of #FairandBalanced response to a hatemonger https://t.co/inPIz0twO6.\n",
            "[ HEY NINCOMPOOPS RESPONSE ]  d(S,CT)= [0.6147604676278882]\n",
            "Social Security and Medicare are not \"Entitlements\" that can be renegotiated.   The term is \"DEFERRED PAY.\"  I've paid into the system my entire life, and so have you. Anyone stealing your deferred pay needs to be voted out.\n",
            "[ ENTITLEMENTS PAID ANYONE ]  d(S,CT)= [0.5906362451764492]\n",
            "'You let a weird guy whisper in your ear and you didn't trust your daughter.' --my 7-year-old girl https://t.co/Iqhx8PUGSo\n",
            "[ WEIRD GUY GIRL ]  d(S,CT)= [0.6341758922936803]\n",
            "@saladinahmed May the initiative to require neutral redistricting in Michigan succeed.   For those unaware of it: https://t.co/DfBydPOnIH.\n",
            "[ MAY REQUIRE MICHIGAN ]  d(S,CT)= [0.6457552298623149]\n",
            "it's interesting watching this with them because I thought Gollum might scare them but they mostly just get upset when he's mistreated.\n",
            "[ INTERESTING SCARE UPSET ]  d(S,CT)= [0.6199618407781209]\n",
            "I have but one request this Christmas, guys. Take the #WarOnChristmas hashtag and fill it with accounts of the horrors of battling elves across snowy fields and firing anti-aircraft missiles at sleds streaking overhead.\n",
            "[ ANTIAIRCRAFT MISSILES SLEDS ]  d(S,CT)= [0.5035085957275477]\n",
            "It is fascinating to see female Democratic senators (with the help of male colleagues) take this coordinated action against Al Franken.\n",
            "[ FASCINATING COLLEAGUES FRANKEN ]  d(S,CT)= [0.6120226878389486]\n",
            "marvel should make a mockumentary about loki as odin and the behind the scenes drama that went on while loki was trying to produce and direct his play. i wanna see tantrums about sets, actors and interviews with asgardians who are ya we know it's loki\n",
            "[ TANTRUMS ACTORS LOKI ]  d(S,CT)= [0.5314089032549687]\n",
            "It seems like a whole bunch of narcissistic sexually harassing dudes who have never had to notice all the people around them who cleaned up their messes are suddenly being handed mops.  I'm all for mailing a few mops to the White House, personally.\n",
            "[ NOTICE PEOPLE SUDDENLY ]  d(S,CT)= [0.5562825975617672]\n"
          ]
        }
      ]
    }
  ]
}